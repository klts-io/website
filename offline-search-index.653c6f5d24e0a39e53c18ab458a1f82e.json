






































































































































[{"body":"Vulnerability details This is a volume security issue related to permission access. A user can access files and directories outside the volume mounting directory, including the host’s file system, through the volume mounting method of subpath in the created container.\nScope This vulnerability affects related behaviors of kubelet, and the issue is particularly serious for cluster administrators who may strictly restrict the creation of hostPath.\nCVSS scores This vulnerability is rated as medium-risk with a CVSS score of 5.5.\nPrevention For users who do not want to upgrade kubelet, they can use two preventative measures:\n Disable VolumeSubpath for kubelet and kube-apiserver and remove all pods that are using this feature. Use admission control to prevent users with low trust levels from running the container with the root permission.  Fixed by official  v1.22.2 v1.21.5 v1.20.11 v1.19.15  Fixed by KLTS  v1.18.20-lts.1 kubernetes/kubernetes#104253 v1.17.17-lts.1 TODO v1.16.15-lts.1 TODO v1.15.12-lts.1 TODO v1.14.10-lts.1 TODO v1.13.12-lts.1 TODO v1.12.10-lts.1 TODO v1.11.10-lts.1 TODO v1.10.13-lts.1 TODO  ","categories":"","description":"","excerpt":"Vulnerability details This is a volume security issue related to …","ref":"/docs/kubernetes/patches/cve-2021-25741/","tags":"","title":"CVE-2021-25741"},{"body":"漏洞详情 这是一个权限访问相关的卷安全性问题。用户可以在创建的容器中通过 subpath 的卷挂载方式访问到该卷挂载目录之外的文件和目录，包括主机的文件系统。\n漏洞影响 该漏洞影响 kubelet 相关行为，对于严格限制 hostPath 创建行为的集群管理员来说问题尤为严重。\n漏洞评分 该漏洞为中危漏洞， CVSS 评分为 5.5。\n防范措施 对于不想升级 kubelet 的用户来说，可以有两种防范措施：\n 禁止 kubelet，kube-apiserver 的 VolumeSubpath 功能，并且移除所有正在使用该功能的 pod。 使用 admission control 防止信任度较低的用户使用 root 权限运行容器。  官方修复的版本  v1.22.2 v1.21.5 v1.20.11 v1.19.15  KLTS 修复的版本  v1.18.20-lts.1 kubernetes/kubernetes#104253 v1.17.17-lts.1 TODO v1.16.15-lts.1 TODO v1.15.12-lts.1 TODO v1.14.10-lts.1 TODO v1.13.12-lts.1 TODO v1.12.10-lts.1 TODO v1.11.10-lts.1 TODO v1.10.13-lts.1 TODO  ","categories":"","description":"","excerpt":"漏洞详情 这是一个权限访问相关的卷安全性问题。用户可以在创建的容器中通过 subpath 的卷挂载方式访问到该卷挂载目录之外的文件和目录，包 …","ref":"/zh/docs/kubernetes/patches/cve-2021-25741/","tags":"","title":"CVE-2021-25741"},{"body":"Vulnerability details A program with this vulnerability may crash because of processing some messages that contain malicious Protobuf. If the version of Gogo Protobuf you are using is too low, this vulnerability may exist.\nScope The Kubernetes system component has its own recovery mechanism to deal with crashes and will not interrupt service when encountering a maliciously submitted Protobuf message, so it is not within the scope affected by the vulnerability.\nWhen the program receives and processes the Protobuf message in the application system, if the component does not have a recovery mechanism to deal with the crash, then such programs are within the scope of the vulnerability, and the service may be interrupted when such a malicious attack comes.\nThe Kubernetes community has tested and verified that API Server is not affected by this vulnerability, but in order to prevent you from being affected by the hidden risk of this security vulnerability, the community has upgraded the relevant Protobuf files.\nPrevention If you use the automatically generated Protobuf message in your application code and find that the relevant component exits due to the following exception, the vulnerability may exist.\npanic: runtime error: index out of range [-9223372036854775804] goroutine 1 [running]: v1.(*MessageName).Unmarshal(0xc00006f1e8, 0xc0000281a8, 0xa, 0x10, 0xc00006f1b8, 0x1) .../protofile.pb.go:250 +0xb86 If you use a component related to the Protobuf message, it is recommended to upgrade the Gogo Protobuf compiler to the bug-fixed version (v1.3.2 or higher), and then regenerate the relevant Protobuf message based on the upgraded Protobuf compiler.\nFixed by official  v1.21.1 v1.20.7 v1.19.11 v1.18.19  Fixed by KLTS  v1.17.17-lts.1 kubernetes/kubernetes#101327 v1.16.15-lts.1 kubernetes/kubernetes#101327 v1.15.12-lts.1 kubernetes/kubernetes#101327 v1.14.10-lts.1 kubernetes/kubernetes#101327 v1.13.12-lts.1 kubernetes/kubernetes#101327 v1.12.10-lts.1 kubernetes/kubernetes#101327 v1.11.10-lts.1 kubernetes/kubernetes#101327 v1.10.13-lts.1 kubernetes/kubernetes#101327  ","categories":"","description":"","excerpt":"Vulnerability details A program with this vulnerability may crash …","ref":"/docs/kubernetes/patches/cve-2021-3121/","tags":"","title":"CVE-2021-3121"},{"body":"漏洞详情 存在该漏洞的程序可能会因为处理了包含恶意 Protobuf 消息而崩溃。如果您使用的 Gogo Protobuf 版本过低，可能存在该漏洞。\n漏洞影响 Kubernetes 系统组件由于自身有应对崩溃的恢复机制，当遇到恶意提交的 Protobuf 消息时不会中断服务，所以不在该漏洞的影响范围内。\n在应用系统中程序接收处理 Protobuf 消息时，如果组件没有应对崩溃的恢复机制，那么这类程序都在该漏洞影响范围内，且被恶意攻击时服务可能会中断。\nKubernetes 社区经过测试验证 API Server 不受该漏洞的影响，但为了避免您受到该安全漏洞隐患的影响，社区对相关 Protobuf 文件进行了升级。\n防范措施 如果在您的应用系统代码中使用了自动生成的 Protobuf 消息，并且发现相关组件因为以下异常退出，则可能存在该漏洞。\npanic: runtime error: index out of range [-9223372036854775804] goroutine 1 [running]: v1.(*MessageName).Unmarshal(0xc00006f1e8, 0xc0000281a8, 0xa, 0x10, 0xc00006f1b8, 0x1) .../protofile.pb.go:250 +0xb86 如果您使用了 Protobuf 消息的相关组件，推荐将 Gogo Protobuf 编译器升级到漏洞修复版本（v1.3.2 或更高的版本），再基于升级后的 Protobuf 编译器重新生成相关的 Protobuf 消息。\n官方修复的版本  v1.21.1 v1.20.7 v1.19.11 v1.18.19  KLTS 修复的版本  v1.17.17-lts.1 kubernetes/kubernetes#101327 v1.16.15-lts.1 kubernetes/kubernetes#101327 v1.15.12-lts.1 kubernetes/kubernetes#101327 v1.14.10-lts.1 kubernetes/kubernetes#101327 v1.13.12-lts.1 kubernetes/kubernetes#101327 v1.12.10-lts.1 kubernetes/kubernetes#101327 v1.11.10-lts.1 kubernetes/kubernetes#101327 v1.10.13-lts.1 kubernetes/kubernetes#101327  ","categories":"","description":"","excerpt":"漏洞详情 存在该漏洞的程序可能会因为处理了包含恶意 Protobuf 消息而崩溃。如果您使用的 Gogo Protobuf 版本过低，可能存 …","ref":"/zh/docs/kubernetes/patches/cve-2021-3121/","tags":"","title":"CVE-2021-3121"},{"body":"Vulnerability details This is a security vulnerability of the kube-apiserver component. An attacker can intercept certain upgrade requests sent to the node kubelet, and forward the request to other target nodes through the original access credentials in the request that could allow an attacker to escalate privileges from a node compromise to a full cluster compromise.\nScope Since kube-apiserver allows the request to be propagated back to the source client in the proxied upgrade request, the attacker can intercept certain upgrade requests sent to the node kubelet, and then use the original access credentials in the request to forward requests to other target nodes, resulting in a privilege escalation vulnerability on the attacked node.\nCVSS scores This vulnerability is rated as medium-risk with a CVSS score of 6.4. If multiple clusters share the same CA and authentication credentials, an attacker can use this vulnerability to attack other clusters. In this case, it is a high-risk vulnerability.\nPrevention For cross-node attacks in the cluster, it is recommended that you take the following preventative measures:\n Timely revoke kubeconfig credentials that may cause leakage potentially, and follow the least principle of minimum permissions to converge unnecessary pods/exec, pods/attach, pods/portforward and proxy resource models with the RBAC permission.  Fixed by official  v1.18.6 v1.17.9 v1.16.13  Fixed by KLTS  v1.15.12-lts.1 kubernetes/kubernetes#92971 v1.14.10-lts.1 kubernetes/kubernetes#92971 v1.13.12-lts.1 TODO v1.12.10-lts.1 TODO v1.11.10-lts.1 TODO v1.10.13-lts.1 TODO  ","categories":"","description":"","excerpt":"Vulnerability details This is a security vulnerability of the …","ref":"/docs/kubernetes/patches/cve-2020-8559/","tags":"","title":"CVE-2020-8559"},{"body":"漏洞详情 这是 kube-apiserver 组件的安全漏洞，攻击者可以通过截取某些发送至节点 kubelet 的升级请求，通过请求中原有的访问凭据转发请求至其他目标节点，从而造成节点权限提升的漏洞。\n漏洞影响 由于 kube-apiserver 在升级请求的代理后端中允许将请求传播回源客户端，所以攻击者可以通过截取某些发送至节点 kubelet 的升级请求，然后利用请求中原有的访问凭据转发请求至其他目标节点，从而造成被攻击节点出现权限提升的漏洞。\n漏洞评分 该漏洞为中危漏洞， CVSS 评分为 6.4。 如果有多个集群共享使用了相同的 CA 和认证凭证，攻击者可以利用此漏洞攻击其他集群，这种情况下该漏洞为高危漏洞。\n防范措施 对于集群内跨节点的攻击，建议您采取以下安全防范措施：\n 及时吊销可能泄露的 kubeconfig 凭证，并且遵循权限最小化原则，收敛子账号不必要的 pods/exec、pods/attach、pods/portforward 和 proxy 类型的资源模型 RBAC 权限。  官方修复的版本  v1.18.6 v1.17.9 v1.16.13  KLTS 修复的版本  v1.15.12-lts.1 kubernetes/kubernetes#92971 v1.14.10-lts.1 kubernetes/kubernetes#92971 v1.13.12-lts.1 TODO v1.12.10-lts.1 TODO v1.11.10-lts.1 TODO v1.10.13-lts.1 TODO  ","categories":"","description":"","excerpt":"漏洞详情 这是 kube-apiserver 组件的安全漏洞，攻击者可以通过截取某些发送至节点 kubelet 的升级请求，通过请求中原有的 …","ref":"/zh/docs/kubernetes/patches/cve-2020-8559/","tags":"","title":"CVE-2020-8559"},{"body":"Vulnerability details The kube-proxy component was found to set the kernel parameter net.ipv4.conf.all.route_localnet=1 in both iptables and ipvs modes to allow local loopback access. An attacker may use the container sharing the host network, or bind and listen to the TCP/UDP service of the local 127.0.0.1 on the cluster node to access the same LAN or adjacent node under the second layer network to obtain interface information. If your service does not set the necessary security certification, it may cause the risk of information leakage.\nScope When an attacker has the capability of configuring host network or can access a container instance with the CAP_NET_RAW capability, he can get the socket service information by listening to 127.0.0.1 on the target node. If there is an exposed service that can be accessed by 127.0.0.1 and does not require any authentication on the target host, then the service information can be obtained by the attacker.\nCVSS scores  If the cluster API Server opens the non-authenticated port (default 8080), then the attacker may obtain information about the API Server interface, the threat level is high-risk vulnerabilities, and the score is 8.8. If the cluster API Server closes non-authenticated ports by default, the threat level is medium-risk vulnerability, and the score is 5.4.  Prevention It is recommended that you take the following preventative measures:\nIf the business container needs to use the host network mode and listen on a non-secure port, you can mitigate this vulnerability by manually adding the iptables rule on the node.\nRun the following command to configure the iptables rule in the cluster to deny non-local access traffic to 127.0.0.1:\niptables -I INPUT --dst 127.0.0.0/8 ! --src 127.0.0.0/8 -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP If the cluster does not need to open the API Server insecure port, you can add --insecure-port=0 to the kubernetes API server command line to disable the port.\nIf untrusted containers are running in the cluster, you can prohibit Container from enabling the CAP_NET_RAW capability, and disable the Container’s CAP_NET_RAW capability in pod spec.\nsecurityContext:capabilities:drop:- \"NET_RAW\"Use PodSecurityPolicy to restrict deployment privileges or shared host network containers. In addition, you can configure requiredDropCapabilities in the policy to force container deployment to close the CAP_NET_RAW capability.\nFixed by official  v1.18.4 v1.17.7 v1.16.11  Fixed by KLTS  v1.15.12-lts.1 kubernetes/kubernetes#92040 v1.14.10-lts.1 kubernetes/kubernetes#92040 v1.13.12-lts.1 kubernetes/kubernetes#92040 v1.12.10-lts.1 CVE-2020-8558.1.12.patch v1.11.10-lts.1 CVE-2020-8558.1.12.patch v1.10.13-lts.1 TODO  ","categories":"","description":"","excerpt":"Vulnerability details The kube-proxy component was found to set the …","ref":"/docs/kubernetes/patches/cve-2020-8558/","tags":"","title":"CVE-2020-8558"},{"body":"漏洞详情 kube-proxy 组件在 iptables 和 ipvs 模式下均需要设置内核参数 net.ipv4.conf.all.route_localnet=1， 从而允许本地回环访问。攻击者可能通过共享主机网络的容器，或在集群节点访问同一个 LAN 或二层网络下的相邻节点上绑定监听本地 127.0.0.1 端口的 TCP/UDP 服务，从而获取接口信息。如果您的服务没有设置必要的安全认证，可能会造成信息泄露风险。\n漏洞影响 当攻击者拥有主机网络配置能力或运行在一个具备了 CAP_NET_RAW 能力的容器实例时，就可以获取在目标节点上监听 127.0.0.1 的服务 socket 信息。如果在目标主机上存在 127.0.0.1 可以访问且不需要任何认证鉴权的暴露服务，那么该服务信息就能被攻击者获取。\n漏洞评分  如果集群 API Server 开启了非认证端口（默认 8080），那么攻击者可能获取到 API Server 接口相关信息，威胁等级为高危漏洞，评分为 8.8 分。 如果集群 API Server 默认关闭了非认证端口，威胁等级为中危漏洞，评分为 5.4 分。  防范措施 建议您采取以下安全防范措施：\n如果业务容器需使用主机网络模式且又在非安全端口上监听，则可以通过在节点上手动添加 iptables 规则来缓解此漏洞。 执行以下命令在集群中配置 iptables 规则，用于拒绝非本地对 127.0.0.1 的访问流量：\niptables -I INPUT --dst 127.0.0.0/8 ! --src 127.0.0.0/8 -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP 如果集群不需要开启 API Server 不安全端口，可以将 --insecure-port=0 添加到 kubernetes API 服务器命令行来禁用端口。\n如集群内运行不受信任的容器，可以禁止 Container 开启 CAP_NET_RAW 能力，在 pod spec 中关闭 Container 的 CAP_NET_RAW 能力。\nsecurityContext:capabilities:drop:- \"NET_RAW\"通过 PodSecurityPolicy 策略限制部署特权或共享主机网络容器，另外可以通过在策略中配置 requiredDropCapabilities 强制容器部署关闭 CAP_NET_RAW 能力。\n官方修复的版本  v1.18.4 v1.17.7 v1.16.11  KLTS 修复的版本  v1.15.12-lts.1 kubernetes/kubernetes#92040 v1.14.10-lts.1 kubernetes/kubernetes#92040 v1.13.12-lts.1 kubernetes/kubernetes#92040 v1.12.10-lts.1 CVE-2020-8558.1.12.patch v1.11.10-lts.1 CVE-2020-8558.1.12.patch v1.10.13-lts.1 TODO  ","categories":"","description":"","excerpt":"漏洞详情 kube-proxy 组件在 iptables 和 ipvs …","ref":"/zh/docs/kubernetes/patches/cve-2020-8558/","tags":"","title":"CVE-2020-8558"},{"body":"Vulnerability details This vulnerability may make the API Server vulnerable to a DoS (Denial of Service) attack caused by successful API requests.\nScope API Server is vulnerable to a denial of service attack via successful API requests.\nFixed by Official  v1.17.3 v1.16.7 v1.15.10  Fixed by KLTS  v1.14.10-lts.1 CVE-2020-8552.1.14.patch v1.13.12-lts.1 CVE-2020-8552.1.13.patch v1.12.10-lts.1 CVE-2020-8552.1.13.patch v1.11.10-lts.1 CVE-2020-8552.1.11.patch v1.10.13-lts.1 CVE-2020-8552.1.11.patch  ","categories":"","description":"","excerpt":"Vulnerability details This vulnerability may make the API Server …","ref":"/docs/kubernetes/patches/cve-2020-8552/","tags":"","title":"CVE-2020-8552"},{"body":"漏洞详情 此漏洞可能使 API Server 易于遭受 API 请求成功造成的 DoS（拒绝服务攻击）。\n漏洞影响 API Server 易于遭受 API 请求成功造成的 DoS（拒绝服务攻击）。\n官方修复的版本  v1.17.3 v1.16.7 v1.15.10  KLTS 修复的版本  v1.14.10-lts.1 CVE-2020-8552.1.14.patch v1.13.12-lts.1 CVE-2020-8552.1.13.patch v1.12.10-lts.1 CVE-2020-8552.1.13.patch v1.11.10-lts.1 CVE-2020-8552.1.11.patch v1.10.13-lts.1 CVE-2020-8552.1.11.patch  ","categories":"","description":"","excerpt":"漏洞详情 此漏洞可能使 API Server 易于遭受 API 请求成功造成的 DoS（拒绝服务攻击）。\n漏洞影响 API Server 易 …","ref":"/zh/docs/kubernetes/patches/cve-2020-8552/","tags":"","title":"CVE-2020-8552"},{"body":"Vulnerability details This vulnerability may allow an attacker to modify or monitor any file in the directory with the same name in the symbolic link header during the unpacking process of the kubectl cp command, thereby causing damage.\nScope The kubectl cp command allows copying files between containers and the user machine. An attacker may implant a malicious tar package with a symbolic link header into the image or running container, and modify or monitor any file in the directory that has the same name as the symbolic link header during the unpacking process of the cp command, thereby causing damage.\nFixed by Official  v1.14.1 v1.13.6 v1.12.8 v1.11.10  Fixed by KLTS  v1.10.13-lts.1 CVE-2019-1002101.1.10.patch  ","categories":"","description":"","excerpt":"Vulnerability details This vulnerability may allow an attacker to …","ref":"/docs/kubernetes/patches/cve-2019-1002101/","tags":"","title":"CVE-2019-1002101"},{"body":"漏洞详情 此漏洞可能允许攻击者在 kubectl cp 命令执行解压过程中修改或监控符号链接头同名目录下的任意文件，从而造成破坏。\n漏洞影响 kubectl cp 命令允许用户在容器和用户机器之间拷贝文件，攻击者可能通过在镜像或运行容器中植入带有符号链接（symbolic links）头的恶意 tar 包，在 cp 命令执行解压过程中修改或监控符号链接头同名目录下的任意文件，从而造成破坏。\n官方修复的版本  v1.14.1 v1.13.6 v1.12.8 v1.11.10  KLTS 修复的版本  v1.10.13-lts.1 CVE-2019-1002101.1.10.patch  ","categories":"","description":"","excerpt":"漏洞详情 此漏洞可能允许攻击者在 kubectl cp 命令执行解压过程中修改或监控符号链接头同名目录下的任意文件，从而造成破坏。 …","ref":"/zh/docs/kubernetes/patches/cve-2019-1002101/","tags":"","title":"CVE-2019-1002101"},{"body":"Vulnerability details This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\nScope This vulnerability has a similar impact caused by CVE-2019-1002101, CVE-2019-11246, and CVE-2019-11249 that were found not long before.\nThe kubectl cp command allows copying files between containers and the user machine. To copy files from a container, Kubernetes runs tar inside the container to create a tar archive, copies it over the network, and kubectl unpacks it on the user’s machine.\nIf the tar binary in the container is malicious, it could run any code and output unexpected, malicious results. An attacker could use this to write files to any path (Path Traversal) on the user’s machine when kubectl cp is called.\nFixed by Official  v1.15.4 v1.14.7 v1.13.11  Fixed by KLTS  v1.12.10-lts.1 kubernetes/kubernetes#82503 v1.11.10-lts.1 kubernetes/kubernetes#82503 v1.10.13-lts.1 CVE-2019-11251.1.10.patch  ","categories":"","description":"","excerpt":"Vulnerability details This vulnerability may allow an attacker to use …","ref":"/docs/kubernetes/patches/cve-2019-11251/","tags":"","title":"CVE-2019-11251"},{"body":"漏洞详情 此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历(Path Traversal)的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n漏洞影响 该漏洞与不久前的 CVE-2019-1002101、CVE-2019-11246、CVE-2019-11249 漏洞影响相似。\nkubectl cp 命令用于用户容器和主机之间的文件拷贝，当从容器中拷贝文件时，Kubernetes 首先会在容器中执行 tar 命令创建相应的归档文件，然后发送给客户端，然后 kubectl 会在用户主机上进行相应解压操作。\n如果容器 tar 包中包含恶意文件，当攻击者具有 kubectl cp 命令的执行权限时，可以利用路径遍历(Path Traversal)。\n官方修复的版本  v1.15.4 v1.14.7 v1.13.11  KLTS 修复的版本  v1.12.10-lts.1 kubernetes/kubernetes#82503 v1.11.10-lts.1 kubernetes/kubernetes#82503 v1.10.13-lts.1 CVE-2019-11251.1.10.patch  ","categories":"","description":"","excerpt":"漏洞详情 此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历(Path Traversal)的方式将容器 tar 包中的恶意 …","ref":"/zh/docs/kubernetes/patches/cve-2019-11251/","tags":"","title":"CVE-2019-11251"},{"body":"Vulnerability details The debugging endpoint /debug/pprof is exposed over the unauthenticated Kubelet healthz port.\nScope The go pprof endpoint is exposed over the Kubelet’s healthz port. This debugging endpoint can potentially leak sensitive information such as internal Kubelet memory addresses and configuration, or for limited denial of service. The issue is of medium severity, but not exposed by the default configuration.\nFixed by Official  v1.14.4 v1.13.8 v1.12.10  Fixed by KLTS  v1.11.10-lts.1 CVE-2019-11248.1.11.patch v1.10.13-lts.1 CVE-2019-11248.1.11.patch  ","categories":"","description":"","excerpt":"Vulnerability details The debugging endpoint /debug/pprof is exposed …","ref":"/docs/kubernetes/patches/cve-2019-11248/","tags":"","title":"CVE-2019-11248"},{"body":"漏洞详情 可以通过健康检查的端口访问 /debug/pprof。\n漏洞影响 该漏洞存在于 Kubelet 中，用于性能调试的 /debug/pprof 跟健康检查端口 /healthz 绑定在一起，其中 /debug/pprof 会进行安全认证，但 /healthz 接口并不进行认证鉴权。所以如果 Kubelet 的 healthz 地址未使用 localhost，则会存在泄露机器敏感信息的风险。\n官方修复的版本  v1.14.4 v1.13.8 v1.12.10  KLTS 修复的版本  v1.11.10-lts.1 CVE-2019-11248.1.11.patch v1.10.13-lts.1 CVE-2019-11248.1.11.patch  ","categories":"","description":"","excerpt":"漏洞详情 可以通过健康检查的端口访问 /debug/pprof。\n漏洞影响 该漏洞存在于 Kubelet 中， …","ref":"/zh/docs/kubernetes/patches/cve-2019-11248/","tags":"","title":"CVE-2019-11248"},{"body":"Vulnerability details API Server mistakenly allows access to a cluster-scoped custom resource.\nScope The Kubernetes kube-apiserver mistakenly allows access to a cluster-scoped custom resource if the request is made as if the resource were namespaced.\nAuthorizations for the resource accessed in this manner are enforced using roles and role bindings within the namespace, meaning that a user with access only to a resource in one namespace could create, view update or delete the cluster-scoped resource (according to their namespace role privileges).\nFixed by Official  v1.12.12  Fixed by KLTS  v1.11.10-lts.1 CVE-2019-11247.1.11.patch v1.10.13-lts.1 TODO  ","categories":"","description":"","excerpt":"Vulnerability details API Server mistakenly allows access to a …","ref":"/docs/kubernetes/patches/cve-2019-11247/","tags":"","title":"CVE-2019-11247"},{"body":"漏洞详情 API Server 允许通过错误的范围访问自定义的资源。\n漏洞影响 如果某个用户请求的资源已划分到命名空间，Kubernetes kube-apiserver 将错误地允许该用户访问集群范围内的自定义资源。\n以这种方式访问资源将等同于授予命名空间内所有角色及其权限，这意味着本来只能访问命名空间中一个资源的用户将能够创建、查看、更新或删除整个集群范围的资源（具体取决于其命名空间角色权限）。\n官方修复的版本  v1.12.12  KLTS 修复的版本  v1.11.10-lts.1 CVE-2019-11247.1.11.patch v1.10.13-lts.1 TODO  ","categories":"","description":"","excerpt":"漏洞详情 API Server 允许通过错误的范围访问自定义的资源。\n漏洞影响 如果某个用户请求的资源已划分到命名空间，Kubernetes …","ref":"/zh/docs/kubernetes/patches/cve-2019-11247/","tags":"","title":"CVE-2019-11247"},{"body":"Vulnerability details This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\nScope This vulnerability has a similar impact caused by CVE-2019-1002101 that was found not long before.\nThe kubectl cp command allows copying files between containers and the user machine. To copy files from a container, Kubernetes runs tar inside the container to create a tar archive, copies it over the network, and kubectl unpacks it on the user’s machine.\nIf the tar binary in the container is malicious, it could run any code and output unexpected, malicious results. An attacker could use this to write files to any path (Path Traversal) on the user’s machine when kubectl cp is called.\nFixed by Official  v1.14.3 v1.13.7 v1.12.10  Fixed by KLTS  v1.11.10-lts.1 CVE-2019-11246.1.11.patch v1.10.13-lts.1 CVE-2019-11246.1.10.patch  ","categories":"","description":"","excerpt":"Vulnerability details This vulnerability may allow an attacker to use …","ref":"/docs/kubernetes/patches/cve-2019-11246/","tags":"","title":"CVE-2019-11246"},{"body":"漏洞详情 此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历 (Path Traversal) 的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n漏洞影响 该漏洞与不久前的 CVE-2019-1002101 漏洞影响相似。\nkubectl cp 命令用于用户容器和主机之间的文件拷贝。当从容器中拷贝文件时，Kubernetes 首先会在容器中执行 tar 命令创建相应的归档文件，然后发送给客户端，kubectl 会在用户主机上进行相应解压操作。\n如果容器 tar 包中包含恶意文件，当攻击者具有 kubectl cp 命令的执行权限时，可以利用路径遍历 (Path Traversal) 方式将恶意文件写入任意路径。\n官方修复的版本  v1.14.3 v1.13.7 v1.12.10  KLTS 修复的版本  v1.11.10-lts.1 CVE-2019-11246.1.11.patch v1.10.13-lts.1 CVE-2019-11246.1.10.patch  ","categories":"","description":"","excerpt":"漏洞详情 此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历 (Path Traversal) 的方式将容器 tar 包中的 …","ref":"/zh/docs/kubernetes/patches/cve-2019-11246/","tags":"","title":"CVE-2019-11246"},{"body":"Vulnerability details Containers for pods that do not specify an explicit runAsUser attempt to run as uid 0 (root) on container restart, or if the image was previously pulled to the node. If the pod specified mustRunAsNonRoot: true, the kubelet will refuse to start the container as root. If the pod did not specify mustRunAsNonRoot: true, the kubelet will run the container as uid 0.\nScope If a pod specifies mustRunAsNonRoot: true, the pod will run as uid 0 when it restarts or the image is pulled on to a node.\nPrevention Specify mustRunAsNonRoot: true for pods.\nFixed by Official  v1.14.3 v1.13.7  Fixed by KLTS  v1.12.10-lts.1 kubernetes/kubernetes#78320 v1.11.10-lts.1 kubernetes/kubernetes#78320 v1.10.13-lts.1 kubernetes/kubernetes#78320  ","categories":"","description":"","excerpt":"Vulnerability details Containers for pods that do not specify an …","ref":"/docs/kubernetes/patches/cve-2019-11245/","tags":"","title":"CVE-2019-11245"},{"body":"漏洞详情 这是一个提权漏洞，通常以容器 Dockerfile 中指定的 USER 运行的容器，在容器重启时或者如果镜像先前被拉到节点时，都将以 root(uid 0) 身份运行。\n漏洞影响 所有未指定 mustRunAsNonRoot: true 的 Pod，在容器重启时或者如果镜像先前被拉到节点时，都将以 root(uid 0) 身份运行。\n防范措施 为 Pod 指定 mustRunAsNonRoot: true。\n官方修复的版本  v1.14.3 v1.13.7  KLTS 修复的版本  v1.12.10-lts.1 kubernetes/kubernetes#78320 v1.11.10-lts.1 kubernetes/kubernetes#78320 v1.10.13-lts.1 kubernetes/kubernetes#78320  ","categories":"","description":"","excerpt":"漏洞详情 这是一个提权漏洞，通常以容器 Dockerfile 中指定的 USER 运行的容器，在容器重启时或者如果镜像先前被拉到节点时，都将 …","ref":"/zh/docs/kubernetes/patches/cve-2019-11245/","tags":"","title":"CVE-2019-11245"},{"body":"Vulnerability details This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\nScope This vulnerability has a similar impact caused by CVE-2019-1002101 and CVE-2019-11246 that were found not long before.\nThe kubectl cp command allows copying files between containers and the user machine. To copy files from a container, Kubernetes runs tar inside the container to create a tar archive, copies it over the network, and kubectl unpacks it on the user’s machine.\nIf the tar binary in the container is malicious, it could run any code and output unexpected, malicious results. An attacker could use this to write files to any path (Path Traversal) on the user’s machine when kubectl cp is called.\nFixed by Official  v1.15.2 v1.14.5 v1.13.9  Fixed by KLTS  v1.12.10-lts.1 CVE-2019-11249.1.12.patch v1.11.10-lts.1 CVE-2019-11249.1.12.patch v1.10.13-lts.1 CVE-2019-11249.1.10.patch  ","categories":"","description":"","excerpt":"Vulnerability details This vulnerability may allow an attacker to use …","ref":"/docs/kubernetes/patches/cve-2019-11249/","tags":"","title":"CVE-2019-11249"},{"body":"漏洞详情 此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历(Path Traversal)的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n漏洞影响 该漏洞与不久前的 CVE-2019-1002101、CVE-2019-11246 漏洞影响相似。\nkubectl cp 命令用于用户容器和主机之间的文件拷贝，当从容器中拷贝文件时，Kubernetes 首先会在容器中执行 tar 命令创建相应的归档文件，然后发送给客户端，然后 kubectl 会在用户主机上进行相应解压操作。\n如果容器 tar 包中包含恶意文件，当攻击者具有 kubectl cp 命令的执行权限时，可以利用路径遍历 (Path Traversal)。\n官方修复的版本  v1.15.2 v1.14.5 v1.13.9  KLTS 修复的版本  v1.12.10-lts.1 CVE-2019-11249.1.12.patch v1.11.10-lts.1 CVE-2019-11249.1.12.patch v1.10.13-lts.1 CVE-2019-11249.1.10.patch  ","categories":"","description":"","excerpt":"漏洞详情 此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历(Path Traversal)的方式将容器 tar 包中的恶意 …","ref":"/zh/docs/kubernetes/patches/cve-2019-11249/","tags":"","title":"CVE-2019-11249"},{"body":"Vulnerability details Vulnerability details(official)\nallowing otherwise unprivileged Linux users to traverse directory contents and execute programs.\nScope When containers included executable programs with extended permission bits (such as setuid), unprivileged Linux users could discover and execute those programs. When the UID of an unprivileged Linux user on the host collided with the file owner or group inside a container, the unprivileged Linux user on the host could discover, read, and modify those files.\nCVSS scores This vulnerability is rated as high-risk with a CVSS score of 7.2.\nPrevention Ensure that the login users of the cluster nodes are all trusted users, and restrict the access rights of untrusted users to the cluster nodes. Remove unnecessary extended permissions in the container bundles directory.\nFixed by Official  v1.4.11 v1.5.7  Fixed by KLTS  v1.3.10-lts.1 CVE-2021-41103.1.3.patch  ","categories":"","description":"","excerpt":"Vulnerability details Vulnerability details(official)\nallowing …","ref":"/docs/containerd/patches/cve-2021-41103/","tags":"","title":"CVE-2021-41103"},{"body":"漏洞详情 漏洞详情(官方)\n在容器根目录和一些系统插件中缺少必要的权限约束时，可使一个非特权的主机 Linux 用户拥有遍历容器文件系统并执行目标程序的权限。\n漏洞影响 在多租户场景下，如果集群节点中的容器包含扩展权限（例如 setuid），非特权的 Linux 用户可能发现并执行该程序。如果非特权的主机 Linux 用户 UID 碰撞到了容器中执行程序的所属用户或组，这个非特权的 Linux 用户可以读写该文件从而导致越权访问。\n漏洞评分 该漏洞为高危漏洞， CVSS 评分为 7.2。\n防范措施 保证集群节点登录用户都是可信用户，限制非受信用户对集群节点的访问权限。 收敛容器 bundles 目录中不必要的扩展权限。\n官方修复的版本  v1.4.11 v1.5.7  KLTS 修复的版本  v1.3.10-lts.1 CVE-2021-41103.1.3.patch  ","categories":"","description":"","excerpt":"漏洞详情 漏洞详情(官方)\n在容器根目录和一些系统插件中缺少必要的权限约束时，可使一个非特权的主机 Linux 用户拥有遍历容器文件系统并执 …","ref":"/zh/docs/containerd/patches/cve-2021-41103/","tags":"","title":"CVE-2021-41103"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.23/","tags":"","title":"v1.23"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.23/","tags":"","title":"v1.23"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.22/","tags":"","title":"v1.22"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.22/","tags":"","title":"v1.22"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.21/","tags":"","title":"v1.21"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.21/","tags":"","title":"v1.21"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.20/","tags":"","title":"v1.20"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.20/","tags":"","title":"v1.20"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.19/","tags":"","title":"v1.19"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.19/","tags":"","title":"v1.19"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.18/","tags":"","title":"v1.18"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.18/","tags":"","title":"v1.18"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.17/","tags":"","title":"v1.17"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.17/","tags":"","title":"v1.17"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.16/","tags":"","title":"v1.16"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.16/","tags":"","title":"v1.16"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.15/","tags":"","title":"v1.15"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.15/","tags":"","title":"v1.15"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.14/","tags":"","title":"v1.14"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.14/","tags":"","title":"v1.14"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.13/","tags":"","title":"v1.13"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.13/","tags":"","title":"v1.13"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.12/","tags":"","title":"v1.12"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.12/","tags":"","title":"v1.12"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.11/","tags":"","title":"v1.11"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.11/","tags":"","title":"v1.11"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/v1.10/","tags":"","title":"v1.10"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/v1.10/","tags":"","title":"v1.10"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/containerd/releases/v1.3/","tags":"","title":"v1.3"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/containerd/releases/v1.3/","tags":"","title":"v1.3"},{"body":"Bug details The node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\nScope When the node is used for a long time, it prompts an error that the remaining space is insufficient. The error message is as follows:\nmkdir: cannot create directory '/sys/fs/cgroup/memory/8': No space left on device The node disk is sufficient but reports this error, and the creation of Pod always fails. This is a potential “time bomb”.\nAll environments that use early-version kernels and Kubernetes versions before 1.21 will be affected. In runc 1.0.0-rc94 (opencontainers/runc#2840) it has been fixed (removed directly).\nPrevention  Upgrade the system kernel Kubernetes 1.14 to 1.20  Rebuild Kubelet with -tags=nokmem   Kubernetes 1.14 or earlier  For hard coding, refer to nokmem.1.13.patch   Kubernetes 1.21 or higher  not affected    Fixed by KLTS  /docs/kubernetes/releases/v1.20/v1.20.15-lts.1/ nokmem.1.20.patch /docs/kubernetes/releases/v1.19/v1.19.16-lts.1/ nokmem.1.20.patch v1.18.20-lts.1 nokmem.1.20.patch v1.17.17-lts.1 nokmem.1.20.patch v1.16.15-lts.1 nokmem.1.20.patch v1.15.12-lts.1 nokmem.1.20.patch v1.14.10-lts.1 nokmem.1.20.patch v1.13.12-lts.1 nokmem.1.13.patch v1.12.10-lts.1 nokmem.1.13.patch v1.11.10-lts.1 nokmem.1.13.patch v1.10.13-lts.1 nokmem.1.13.patch  ","categories":"","description":"","excerpt":"Bug details The node has sufficient disks, but it keeps reporting that …","ref":"/docs/kubernetes/patches/nokmem/","tags":"","title":"nokmem"},{"body":"Bug 详情 节点磁盘充足但是一直报磁盘不足无法创建 Pod。\nBug 影响 节点长期使用的时候提示剩余空间不足的错误，报错信息如下所示：\nmkdir: cannot create directory '/sys/fs/cgroup/memory/8': No space left on device 节点磁盘充足但是一直报和这个错误, 并且创建 Pod 总是失败，这是一个潜在的“定时炸弹”。\n所有使用低版本内核的环境以及 Kubernetes 1.21 之前的版本都会受到影响，在 runc 1.0.0-rc94 (opencontainers/runc#2840) 进行了修复(被直接移除)。\n防范措施  升级系统内核 Kubernetes 1.14 到 1.20  重新构建 Kubelet 带上 -tags=nokmem   Kubernetes 1.14 以下  有关硬编码，请参考 nokmem.1.13.patch   Kubernetes 1.21 及以上  不受影响    KLTS 修复的版本  /docs/kubernetes/releases/v1.20/v1.20.15-lts.1/ nokmem.1.20.patch /docs/kubernetes/releases/v1.19/v1.19.16-lts.1/ nokmem.1.20.patch v1.18.20-lts.1 nokmem.1.20.patch v1.17.17-lts.1 nokmem.1.20.patch v1.16.15-lts.1 nokmem.1.20.patch v1.15.12-lts.1 nokmem.1.20.patch v1.14.10-lts.1 nokmem.1.20.patch v1.13.12-lts.1 nokmem.1.13.patch v1.12.10-lts.1 nokmem.1.13.patch v1.11.10-lts.1 nokmem.1.13.patch v1.10.13-lts.1 nokmem.1.13.patch  ","categories":"","description":"","excerpt":"Bug 详情 节点磁盘充足但是一直报磁盘不足无法创建 Pod。\nBug 影响 节点长期使用的时候提示剩余空间不足的错误，报错信息如下所示： …","ref":"/zh/docs/kubernetes/patches/nokmem/","tags":"","title":"nokmem"},{"body":"This is the first fixed release by KLTS for v1.10.13.\nPatches  CVE-2019-11245Containers for pods that do not specify an explicit runAsUser attempt to run as uid 0 (root) on container restart, or if the image was previously pulled to the node.\n CVE-2019-1002101This vulnerability may allow an attacker to modify or monitor any file in the directory with the same name in the symbolic link header during the unpacking process of the kubectl cp command, thereby causing damage.\n CVE-2019-11246This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\n TODO CVE-2019-11247API Server mistakenly allows access to a cluster-scoped custom resource.\n CVE-2019-11248The debugging endpoint /debug/pprof is exposed over the unauthenticated Kubelet healthz port.\n CVE-2019-11249This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\n CVE-2019-11251This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\n CVE-2020-8552This vulnerability may make the API Server vulnerable to a DoS (Denial of Service) attack caused by successful API requests.\n TODO CVE-2020-8558The kube-proxy component was found to set the kernel parameter net.ipv4.conf.all.route_localnet=1 in both iptables and ipvs modes to allow local loopback access. An attacker may use the container sharing the host network, or bind and listen to the TCP/UDP service of the local 127.0.0.1 on the cluster node to access the same LAN or adjacent node under the second layer network to obtain interface information. If your service does not set the necessary security certification, it may cause the risk of information leakage.\n TODO CVE-2020-8559This is a security vulnerability of the kube-apiserver component. An attacker can intercept certain upgrade requests sent to the node kubelet, and forward the request to other target nodes through the original access credentials in the request that could allow an attacker to escalate privileges from a node compromise to a full cluster compromise.\n CVE-2021-3121A program with this vulnerability may crash because of processing some messages that contain malicious Protobuf. If the version of Gogo Protobuf you are using is too low, this vulnerability may exist.\n nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.10.13.\nPatches …","ref":"/docs/kubernetes/releases/v1.10/v1.10.13-lts.1/","tags":"","title":"v1.10.13-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.10.13 修复版本。\n补丁  CVE-2019-11245这是一个提权漏洞，通常以容器 Dockerfile 中指定的 USER 运行的容器，在容器重启时或者如果镜像先前被拉到节点时，都将以 root(uid 0) 身份运行。\n CVE-2019-1002101此漏洞可能允许攻击者在 kubectl cp 命令执行解压过程中修改或监控符号链接头同名目录下的任意文件，从而造成破坏。\n CVE-2019-11246此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历 (Path Traversal) 的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n TODO CVE-2019-11247API Server 允许通过错误的范围访问自定义的资源。\n CVE-2019-11248可以通过健康检查的端口访问 /debug/pprof。\n CVE-2019-11249此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历(Path Traversal)的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n CVE-2019-11251此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历(Path Traversal)的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n CVE-2020-8552此漏洞可能使 API Server 易于遭受 API 请求成功造成的 DoS（拒绝服务攻击）。\n TODO CVE-2020-8558kube-proxy 组件在 iptables 和 ipvs 模式下均需要设置内核参数 net.ipv4.conf.all.route_localnet=1， 从而允许本地回环访问。攻击者可能通过共享主机网络的容器，或在集群节点访问同一个 LAN 或二层网络下的相邻节点上绑定监听本地 127.0.0.1 端口的 TCP/UDP 服务，从而获取接口信息。如果您的服务没有设置必要的安全认证，可能会造成信息泄露风险。\n TODO CVE-2020-8559这是 kube-apiserver 组件的安全漏洞，攻击者可以通过截取某些发送至节点 kubelet 的升级请求，通过请求中原有的访问凭据转发请求至其他目标节点，从而造成节点权限提升的漏洞。\n CVE-2021-3121存在该漏洞的程序可能会因为处理了包含恶意 Protobuf 消息而崩溃。如果您使用的 Gogo Protobuf 版本过低，可能存在该漏洞。\n nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.10.13 修复版本。\n补丁  CVE-2019-11245这是一个提权漏洞， …","ref":"/zh/docs/kubernetes/releases/v1.10/v1.10.13-lts.1/","tags":"","title":"v1.10.13-lts.1"},{"body":"This is the first fixed release by KLTS for v1.11.10.\nPatches  CVE-2019-11245Containers for pods that do not specify an explicit runAsUser attempt to run as uid 0 (root) on container restart, or if the image was previously pulled to the node.\n CVE-2019-11246This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\n CVE-2019-11247API Server mistakenly allows access to a cluster-scoped custom resource.\n CVE-2019-11248The debugging endpoint /debug/pprof is exposed over the unauthenticated Kubelet healthz port.\n CVE-2019-11249This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\n CVE-2019-11251This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\n CVE-2020-8552This vulnerability may make the API Server vulnerable to a DoS (Denial of Service) attack caused by successful API requests.\n CVE-2020-8558The kube-proxy component was found to set the kernel parameter net.ipv4.conf.all.route_localnet=1 in both iptables and ipvs modes to allow local loopback access. An attacker may use the container sharing the host network, or bind and listen to the TCP/UDP service of the local 127.0.0.1 on the cluster node to access the same LAN or adjacent node under the second layer network to obtain interface information. If your service does not set the necessary security certification, it may cause the risk of information leakage.\n TODO CVE-2020-8559This is a security vulnerability of the kube-apiserver component. An attacker can intercept certain upgrade requests sent to the node kubelet, and forward the request to other target nodes through the original access credentials in the request that could allow an attacker to escalate privileges from a node compromise to a full cluster compromise.\n CVE-2021-3121A program with this vulnerability may crash because of processing some messages that contain malicious Protobuf. If the version of Gogo Protobuf you are using is too low, this vulnerability may exist.\n nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.11.10.\nPatches …","ref":"/docs/kubernetes/releases/v1.11/v1.11.10-lts.1/","tags":"","title":"v1.11.10-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.11.10 修复版本。\n补丁  CVE-2019-11245这是一个提权漏洞，通常以容器 Dockerfile 中指定的 USER 运行的容器，在容器重启时或者如果镜像先前被拉到节点时，都将以 root(uid 0) 身份运行。\n CVE-2019-11246此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历 (Path Traversal) 的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n CVE-2019-11247API Server 允许通过错误的范围访问自定义的资源。\n CVE-2019-11248可以通过健康检查的端口访问 /debug/pprof。\n CVE-2019-11249此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历(Path Traversal)的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n CVE-2019-11251此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历(Path Traversal)的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n CVE-2020-8552此漏洞可能使 API Server 易于遭受 API 请求成功造成的 DoS（拒绝服务攻击）。\n CVE-2020-8558kube-proxy 组件在 iptables 和 ipvs 模式下均需要设置内核参数 net.ipv4.conf.all.route_localnet=1， 从而允许本地回环访问。攻击者可能通过共享主机网络的容器，或在集群节点访问同一个 LAN 或二层网络下的相邻节点上绑定监听本地 127.0.0.1 端口的 TCP/UDP 服务，从而获取接口信息。如果您的服务没有设置必要的安全认证，可能会造成信息泄露风险。\n TODO CVE-2020-8559这是 kube-apiserver 组件的安全漏洞，攻击者可以通过截取某些发送至节点 kubelet 的升级请求，通过请求中原有的访问凭据转发请求至其他目标节点，从而造成节点权限提升的漏洞。\n CVE-2021-3121存在该漏洞的程序可能会因为处理了包含恶意 Protobuf 消息而崩溃。如果您使用的 Gogo Protobuf 版本过低，可能存在该漏洞。\n nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.11.10 修复版本。\n补丁  CVE-2019-11245这是一个提权漏洞， …","ref":"/zh/docs/kubernetes/releases/v1.11/v1.11.10-lts.1/","tags":"","title":"v1.11.10-lts.1"},{"body":"This is the first fixed release by KLTS for v1.12.10.\nPatches  CVE-2019-11245Containers for pods that do not specify an explicit runAsUser attempt to run as uid 0 (root) on container restart, or if the image was previously pulled to the node.\n CVE-2019-11247API Server mistakenly allows access to a cluster-scoped custom resource.\n CVE-2019-11249This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\n CVE-2019-11251This vulnerability may allow an attacker to use the kubectl cp command to write malicious files in the container tar package to any path on the host using Path Traversal. This process is limited only by the system permissions of the local user.\n CVE-2020-8552This vulnerability may make the API Server vulnerable to a DoS (Denial of Service) attack caused by successful API requests.\n CVE-2020-8558The kube-proxy component was found to set the kernel parameter net.ipv4.conf.all.route_localnet=1 in both iptables and ipvs modes to allow local loopback access. An attacker may use the container sharing the host network, or bind and listen to the TCP/UDP service of the local 127.0.0.1 on the cluster node to access the same LAN or adjacent node under the second layer network to obtain interface information. If your service does not set the necessary security certification, it may cause the risk of information leakage.\n TODO CVE-2020-8559This is a security vulnerability of the kube-apiserver component. An attacker can intercept certain upgrade requests sent to the node kubelet, and forward the request to other target nodes through the original access credentials in the request that could allow an attacker to escalate privileges from a node compromise to a full cluster compromise.\n CVE-2021-3121A program with this vulnerability may crash because of processing some messages that contain malicious Protobuf. If the version of Gogo Protobuf you are using is too low, this vulnerability may exist.\n nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.12.10.\nPatches …","ref":"/docs/kubernetes/releases/v1.12/v1.12.10-lts.1/","tags":"","title":"v1.12.10-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.12.10 修复版本。\n补丁  CVE-2019-11245这是一个提权漏洞，通常以容器 Dockerfile 中指定的 USER 运行的容器，在容器重启时或者如果镜像先前被拉到节点时，都将以 root(uid 0) 身份运行。\n CVE-2019-11247API Server 允许通过错误的范围访问自定义的资源。\n CVE-2019-11249此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历(Path Traversal)的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n CVE-2019-11251此漏洞可能允许攻击者利用 kubectl cp 命令，采用路径遍历(Path Traversal)的方式将容器 tar 包中的恶意文件写入所在主机上的任何路径，该过程仅受本地用户的系统权限限制。\n CVE-2020-8552此漏洞可能使 API Server 易于遭受 API 请求成功造成的 DoS（拒绝服务攻击）。\n CVE-2020-8558kube-proxy 组件在 iptables 和 ipvs 模式下均需要设置内核参数 net.ipv4.conf.all.route_localnet=1， 从而允许本地回环访问。攻击者可能通过共享主机网络的容器，或在集群节点访问同一个 LAN 或二层网络下的相邻节点上绑定监听本地 127.0.0.1 端口的 TCP/UDP 服务，从而获取接口信息。如果您的服务没有设置必要的安全认证，可能会造成信息泄露风险。\n TODO CVE-2020-8559这是 kube-apiserver 组件的安全漏洞，攻击者可以通过截取某些发送至节点 kubelet 的升级请求，通过请求中原有的访问凭据转发请求至其他目标节点，从而造成节点权限提升的漏洞。\n CVE-2021-3121存在该漏洞的程序可能会因为处理了包含恶意 Protobuf 消息而崩溃。如果您使用的 Gogo Protobuf 版本过低，可能存在该漏洞。\n nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.12.10 修复版本。\n补丁  CVE-2019-11245这是一个提权漏洞， …","ref":"/zh/docs/kubernetes/releases/v1.12/v1.12.10-lts.1/","tags":"","title":"v1.12.10-lts.1"},{"body":"This is the first fixed release by KLTS for v1.13.12.\nPatches  CVE-2020-8552This vulnerability may make the API Server vulnerable to a DoS (Denial of Service) attack caused by successful API requests.\n CVE-2020-8558The kube-proxy component was found to set the kernel parameter net.ipv4.conf.all.route_localnet=1 in both iptables and ipvs modes to allow local loopback access. An attacker may use the container sharing the host network, or bind and listen to the TCP/UDP service of the local 127.0.0.1 on the cluster node to access the same LAN or adjacent node under the second layer network to obtain interface information. If your service does not set the necessary security certification, it may cause the risk of information leakage.\n TODO CVE-2020-8559This is a security vulnerability of the kube-apiserver component. An attacker can intercept certain upgrade requests sent to the node kubelet, and forward the request to other target nodes through the original access credentials in the request that could allow an attacker to escalate privileges from a node compromise to a full cluster compromise.\n CVE-2021-3121A program with this vulnerability may crash because of processing some messages that contain malicious Protobuf. If the version of Gogo Protobuf you are using is too low, this vulnerability may exist.\n nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.13.12.\nPatches …","ref":"/docs/kubernetes/releases/v1.13/v1.13.12-lts.1/","tags":"","title":"v1.13.12-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.13.12 修复版本。\n补丁  CVE-2020-8552此漏洞可能使 API Server 易于遭受 API 请求成功造成的 DoS（拒绝服务攻击）。\n CVE-2020-8558kube-proxy 组件在 iptables 和 ipvs 模式下均需要设置内核参数 net.ipv4.conf.all.route_localnet=1， 从而允许本地回环访问。攻击者可能通过共享主机网络的容器，或在集群节点访问同一个 LAN 或二层网络下的相邻节点上绑定监听本地 127.0.0.1 端口的 TCP/UDP 服务，从而获取接口信息。如果您的服务没有设置必要的安全认证，可能会造成信息泄露风险。\n TODO CVE-2020-8559这是 kube-apiserver 组件的安全漏洞，攻击者可以通过截取某些发送至节点 kubelet 的升级请求，通过请求中原有的访问凭据转发请求至其他目标节点，从而造成节点权限提升的漏洞。\n CVE-2021-3121存在该漏洞的程序可能会因为处理了包含恶意 Protobuf 消息而崩溃。如果您使用的 Gogo Protobuf 版本过低，可能存在该漏洞。\n nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.13.12 修复版本。\n补丁  CVE-2020-8552此漏洞可能使 API Server …","ref":"/zh/docs/kubernetes/releases/v1.13/v1.13.12-lts.1/","tags":"","title":"v1.13.12-lts.1"},{"body":"This is the first fixed release by KLTS for v1.14.10.\nPatches  CVE-2020-8552This vulnerability may make the API Server vulnerable to a DoS (Denial of Service) attack caused by successful API requests.\n CVE-2020-8558The kube-proxy component was found to set the kernel parameter net.ipv4.conf.all.route_localnet=1 in both iptables and ipvs modes to allow local loopback access. An attacker may use the container sharing the host network, or bind and listen to the TCP/UDP service of the local 127.0.0.1 on the cluster node to access the same LAN or adjacent node under the second layer network to obtain interface information. If your service does not set the necessary security certification, it may cause the risk of information leakage.\n CVE-2020-8559This is a security vulnerability of the kube-apiserver component. An attacker can intercept certain upgrade requests sent to the node kubelet, and forward the request to other target nodes through the original access credentials in the request that could allow an attacker to escalate privileges from a node compromise to a full cluster compromise.\n CVE-2021-3121A program with this vulnerability may crash because of processing some messages that contain malicious Protobuf. If the version of Gogo Protobuf you are using is too low, this vulnerability may exist.\n nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.14.10.\nPatches …","ref":"/docs/kubernetes/releases/v1.14/v1.14.10-lts.1/","tags":"","title":"v1.14.10-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.14.10 修复版本。\n补丁  CVE-2020-8552此漏洞可能使 API Server 易于遭受 API 请求成功造成的 DoS（拒绝服务攻击）。\n CVE-2020-8558kube-proxy 组件在 iptables 和 ipvs 模式下均需要设置内核参数 net.ipv4.conf.all.route_localnet=1， 从而允许本地回环访问。攻击者可能通过共享主机网络的容器，或在集群节点访问同一个 LAN 或二层网络下的相邻节点上绑定监听本地 127.0.0.1 端口的 TCP/UDP 服务，从而获取接口信息。如果您的服务没有设置必要的安全认证，可能会造成信息泄露风险。\n CVE-2020-8559这是 kube-apiserver 组件的安全漏洞，攻击者可以通过截取某些发送至节点 kubelet 的升级请求，通过请求中原有的访问凭据转发请求至其他目标节点，从而造成节点权限提升的漏洞。\n CVE-2021-3121存在该漏洞的程序可能会因为处理了包含恶意 Protobuf 消息而崩溃。如果您使用的 Gogo Protobuf 版本过低，可能存在该漏洞。\n nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.14.10 修复版本。\n补丁  CVE-2020-8552此漏洞可能使 API Server …","ref":"/zh/docs/kubernetes/releases/v1.14/v1.14.10-lts.1/","tags":"","title":"v1.14.10-lts.1"},{"body":"This is the first fixed release by KLTS for v1.15.12.\nPatches  CVE-2020-8558The kube-proxy component was found to set the kernel parameter net.ipv4.conf.all.route_localnet=1 in both iptables and ipvs modes to allow local loopback access. An attacker may use the container sharing the host network, or bind and listen to the TCP/UDP service of the local 127.0.0.1 on the cluster node to access the same LAN or adjacent node under the second layer network to obtain interface information. If your service does not set the necessary security certification, it may cause the risk of information leakage.\n CVE-2021-3121A program with this vulnerability may crash because of processing some messages that contain malicious Protobuf. If the version of Gogo Protobuf you are using is too low, this vulnerability may exist.\n nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.15.12.\nPatches …","ref":"/docs/kubernetes/releases/v1.15/v1.15.12-lts.1/","tags":"","title":"v1.15.12-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.15.12 修复版本。\n补丁  CVE-2020-8558kube-proxy 组件在 iptables 和 ipvs 模式下均需要设置内核参数 net.ipv4.conf.all.route_localnet=1， 从而允许本地回环访问。攻击者可能通过共享主机网络的容器，或在集群节点访问同一个 LAN 或二层网络下的相邻节点上绑定监听本地 127.0.0.1 端口的 TCP/UDP 服务，从而获取接口信息。如果您的服务没有设置必要的安全认证，可能会造成信息泄露风险。\n CVE-2021-3121存在该漏洞的程序可能会因为处理了包含恶意 Protobuf 消息而崩溃。如果您使用的 Gogo Protobuf 版本过低，可能存在该漏洞。\n nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.15.12 修复版本。\n补丁  CVE-2020-8558kube-proxy 组件在 iptables …","ref":"/zh/docs/kubernetes/releases/v1.15/v1.15.12-lts.1/","tags":"","title":"v1.15.12-lts.1"},{"body":"This is the first fixed release by KLTS for v1.16.15.\nPatches  CVE-2021-3121A program with this vulnerability may crash because of processing some messages that contain malicious Protobuf. If the version of Gogo Protobuf you are using is too low, this vulnerability may exist.\n nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.16.15.\nPatches …","ref":"/docs/kubernetes/releases/v1.16/v1.16.15-lts.1/","tags":"","title":"v1.16.15-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.16.15 修复版本。\n补丁  CVE-2021-3121存在该漏洞的程序可能会因为处理了包含恶意 Protobuf 消息而崩溃。如果您使用的 Gogo Protobuf 版本过低，可能存在该漏洞。\n nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.16.15 修复版本。\n补丁  CVE-2021-3121 …","ref":"/zh/docs/kubernetes/releases/v1.16/v1.16.15-lts.1/","tags":"","title":"v1.16.15-lts.1"},{"body":"This is the first fixed release by KLTS for v1.17.17.\nPatches  CVE-2021-3121A program with this vulnerability may crash because of processing some messages that contain malicious Protobuf. If the version of Gogo Protobuf you are using is too low, this vulnerability may exist.\n nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.17.17.\nPatches …","ref":"/docs/kubernetes/releases/v1.17/v1.17.17-lts.1/","tags":"","title":"v1.17.17-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.17.17 修复版本。\n补丁  CVE-2021-3121存在该漏洞的程序可能会因为处理了包含恶意 Protobuf 消息而崩溃。如果您使用的 Gogo Protobuf 版本过低，可能存在该漏洞。\n nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.17.17 修复版本。\n补丁  CVE-2021-3121 …","ref":"/zh/docs/kubernetes/releases/v1.17/v1.17.17-lts.1/","tags":"","title":"v1.17.17-lts.1"},{"body":"This is the first fixed release by KLTS for v1.18.20.\nPatches  nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n CVE-2021-25741This is a volume security issue related to permission access. A user can access files and directories outside the volume mounting directory, including the host’s file system, through the volume mounting method of subpath in the created container.\n  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.18.20.\nPatches …","ref":"/docs/kubernetes/releases/v1.18/v1.18.20-lts.1/","tags":"","title":"v1.18.20-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.18.20 修复版本。\n补丁  nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n CVE-2021-25741这是一个权限访问相关的卷安全性问题。用户可以在创建的容器中通过 subpath 的卷挂载方式访问到该卷挂载目录之外的文件和目录，包括主机的文件系统。\n  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.18.20 修复版本。\n补丁  nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。 …","ref":"/zh/docs/kubernetes/releases/v1.18/v1.18.20-lts.1/","tags":"","title":"v1.18.20-lts.1"},{"body":"This is the second fixed release by KLTS for v1.18.20.\nPatches  Bugfix: reducing race risk in kubelet for missing KUBERNETES_SERVICE_HOST  ","categories":"","description":"","excerpt":"This is the second fixed release by KLTS for v1.18.20.\nPatches …","ref":"/docs/kubernetes/releases/v1.18/v1.18.20-lts.2/","tags":"","title":"v1.18.20-lts.2"},{"body":"这是 KLTS 发布的第二个 v1.18.20 修复版本。\n补丁  修复问题: kubelet 重启后创建的新 pod 缺少环境变量 KUBERNETES_SERVICE_HOST  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第二个 v1.18.20 修复版本。\n补丁  修复问题: kubelet 重启后创建的新 pod …","ref":"/zh/docs/kubernetes/releases/v1.18/v1.18.20-lts.2/","tags":"","title":"v1.18.20-lts.2"},{"body":"This is the third fixed release by KLTS for v1.19.16.\nPatches  nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n /docs/kubernetes/patches/cve-2020-8554/  ","categories":"","description":"","excerpt":"This is the third fixed release by KLTS for v1.19.16.\nPatches …","ref":"/docs/kubernetes/releases/v1.19/v1.19.16-lts.3/","tags":"","title":"v1.19.16-lts.3"},{"body":"这是 KLTS 发布的第三个 v1.19.16 修复版本。\n补丁  nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n /docs/kubernetes/patches/cve-2020-8554/  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第三个 v1.19.16 修复版本。\n补丁  nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。 …","ref":"/zh/docs/kubernetes/releases/v1.19/v1.19.16-lts.3/","tags":"","title":"v1.19.16-lts.3"},{"body":"这是 KLTS 发布的第二个 v1.20.15 修复版本。\n补丁  nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第二个 v1.20.15 修复版本。\n补丁  nokmem节点磁盘充足但是一直报磁盘不足无法创建 Pod。\n  ","ref":"/zh/docs/kubernetes/releases/v1.20/v1.20.15-lts.2/","tags":"","title":"v1.20.15-lts.1"},{"body":"This is the second fixed release by KLTS for v1.20.15.\nPatches  nokmemThe node has sufficient disks, but it keeps reporting that the disk is insufficient to create a Pod.\n  ","categories":"","description":"","excerpt":"This is the second fixed release by KLTS for v1.20.15.\nPatches …","ref":"/docs/kubernetes/releases/v1.20/v1.20.15-lts.2/","tags":"","title":"v1.20.15-lts.2"},{"body":"This is the first fixed release by KLTS for v1.21.11.\nPatches  There are no fixes just CI processes running  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.21.11.\nPatches  There …","ref":"/docs/kubernetes/releases/v1.21/v1.21.11-lts.1/","tags":"","title":"v1.21.11-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.21.11 修复版本。\n补丁  只是跑通了 CI 没做任何修复  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.21.11 修复版本。\n补丁  只是跑通了 CI 没做任何修复  ","ref":"/zh/docs/kubernetes/releases/v1.21/v1.21.11-lts.1/","tags":"","title":"v1.21.11-lts.1"},{"body":"This is the first fixed release by KLTS for v1.22.8.\nPatches  There are no fixes just CI processes running  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.22.8.\nPatches  There …","ref":"/docs/kubernetes/releases/v1.22/v1.22.8-lts.1/","tags":"","title":"v1.22.8-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.22.8 修复版本。\n补丁  只是跑通了 CI 没做任何修复  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.22.8 修复版本。\n补丁  只是跑通了 CI 没做任何修复  ","ref":"/zh/docs/kubernetes/releases/v1.22/v1.22.8-lts.1/","tags":"","title":"v1.22.8-lts.1"},{"body":"这是 KLTS 发布的第一个 v1.23.5 修复版本。\n补丁  只是跑通了 CI 没做任何修复  ","categories":"","description":"","excerpt":"这是 KLTS 发布的第一个 v1.23.5 修复版本。\n补丁  只是跑通了 CI 没做任何修复  ","ref":"/zh/docs/kubernetes/releases/v1.23/v1.23.5-lts.1/","tags":"","title":"v1.23.4-lts.1"},{"body":"This is the first fixed release by KLTS for v1.23.5.\nPatches  There are no fixes just CI processes running  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS for v1.23.5.\nPatches  There …","ref":"/docs/kubernetes/releases/v1.23/v1.23.5-lts.1/","tags":"","title":"v1.23.5-lts.1"},{"body":"This is the first fixed release by KLTS Containerd for v1.3.10.\nPatches  CVE-2021-41103Where container root directories and some plugins had insufficiently restricted permissions, allowing otherwise unprivileged Linux users to traverse directory contents and execute programs.\n  ","categories":"","description":"","excerpt":"This is the first fixed release by KLTS Containerd for v1.3.10. …","ref":"/docs/containerd/releases/v1.3/v1.3.10-lts.1/","tags":"","title":"v1.3.10-lts.1"},{"body":"这是 KLTS Containerd 发布的第一个 v1.3.10 修复版本。\n补丁  CVE-2021-41103在容器根目录和一些系统插件中缺少必要的权限约束时，可使一个非特权的主机 Linux 用户拥有遍历容器文件系统并执行目标程序的权限。\n  ","categories":"","description":"","excerpt":"这是 KLTS Containerd 发布的第一个 v1.3.10 修复版本。\n补丁  CVE-2021-41103在容器根目录和一些系统插 …","ref":"/zh/docs/containerd/releases/v1.3/v1.3.10-lts.1/","tags":"","title":"v1.3.10-lts.1"},{"body":"KLTS, known as Kubernetes Long Term Support, has a primary mission to provide free long-term maintenance support for early versions of Kubernetes.\nOne of the reasons to maintain early versions is the fact that in a real production environment, the latest release is not the best or the most stable. In a normal case, a of Kubernetes is not available until one year after the initial release of a particular version. For details see Kubernetes release cycle. After the community aborts maintenance, KLTS will continue to maintain it in the next three years.\nWhy is the choice of most enterprises today to stay with early versions and not rush to upgrade?\n  Firstly, the high frequency of upgrades may cause more risks and each upgrade must be fully verified. In the financial industry, the change cycle of a PaaS platform is usually relatively long, because once the updated version has a bug, it needs to be forced to roll back or quickly respond and upgrade to a updated version, which will cause unnecessary expenditures.\n  Secondly, once an enterprise upgrades their the Kubernetes kernel, some functional alternatives may not yet fully ready for production, and incompatibility often occurs in the production environment.\n  Finally, the Kubernetes community only supports minor version upgrades one by one, and does not support cross-version upgrades, because the later upgrades often have some uncontrollable factors that may cause some production problems.\n  Therefore, the choice of most enterprises today is to stay with early versions and not rush to upgrade. But the Kubernetes community only maintains the most recent three to four releases, how can you keep early versions safe from the CVE bugs and vulnerabilities that the community may discover from time to time? That’s where KLTS comes in! We provide free maintenance support for early versions for up to three years, actively fix the CVE vulnerabilities and critical bugs.\nKLTS release cycle Kubernetes versions are expressed as x.y.z, where x is the major version, y is the minor version, and z is the patch version. For the versions maintained by KLTS, it is followed by a string beginning with lts. For the convenience of communication, people often use the first two digits x.y to describe the Kubernetes version.\nAssuming that the latest Kubernetes released by the community is x.y, according to the Version Skew Policy, the community only maintains the latest three versions, and KLTS currently maintains nearly ten early versions starting from 1.10, as shown in the figure below.\nWhen the Kubernetes community discovers new CVE vulnerabilities or bugs that may affect production, it may be affected not only the versions that the community is maintaining, but also the early versions that have been discontinued before but are still in use by some enterprises and cannot be upgraded rashly, which are maintained by the KLTS team. The current maintenance cycle of KLTS is as follows:\nAs shown above, the maintenance cycle of a certain version by the Kubernetes community is usually about one year, and KLTS can provide a long-term maintenance in the next three years until the code is incompatible, and then the corresponding version will be aborted.\nBugs fixed by KLTS Some high-priority CVEs or serious bugs in the production environment may cause greater security risks. CVE security issues are the lifeblood of the cluster, so KLTS will fix mid-to-high-risk CVEs, and then fix major bugs to guarantee stable operation of the production environment.\nAs an example, the CVE-2021-3121 vulnerability discovered in January 2021 has a CVSS score of 7.5. However, as of September 2021 the Kubernetes community:\n Only fixed four versions: 1.18, 1.19, 1.20, 1.21 Announced that “all prior versions remain exposed and users should stop using them immediately” Requests to fix early versions are denied:  KLTS addresses this situation by diligently fixing eight earlier versions that were heavily affected by the CVE-2021-3121 vulnerability. No complaints, no demands!\n v1.17.17 v1.16.15 v1.15.12 v1.14.10 v1.13.12 v1.12.10 v1.11.10 v1.10.13  If you feel that the KLTS team’s efforts are valuable and interesting to you, don’t hesitate to join the KLTS community to talk and contribute.\nWelcome to talk and join After careful maintenance and diligent support by developers, KLTS brings the following results to these early versions:\n Three-year maintenance period: KLTS will provide continuous maintenance for up to three years once the Kubernetes community aborts maintenance. Safe and stable: Minor version upgrades are safer with high compatibility. This kind of progressive upgrade is more stable. For example, the new features provided by the latest version may be very attractive, but it may not be able to meet the standards available for production and require a long time to adapt. Easy to install: Combined with domestic image acceleration, it natively supports Kubeadm, and CentOS, Ubuntu, and openSUSE, and also provides a one-click installation script. Open and transparent: KLTS is an open source project hosted on GitHub and the whole process is open. more in roadmap: Long-term maintenance of Containerd and other components will be added later.  Here is a sincere invitation to developers. If you feel that the KLTS team’s contributions are valuable and make you trustworthy, you are welcome to join the KLTS community to talk and contribute. We look forward to any comments, suggestions, or solutions.\n KLTS community KLTS Slack channel  Kubernetes release cycle The release cycle of recent ten versions by the Kubernetes community are as follows:\n   Ver. Initial date EOL date     1.10 2018-03-27 2019-02-13   1.11 2018-07-28 2019-05-01   1.12 2018-09-28 2019-07-08   1.13 2018-12-04 2019-10-15   1.14 2019-03-25 2019-12-11   1.15 2019-07-20 2020-05-06   1.16 2019-09-18 2020-09-02   1.17 2019-12-08 2021-01-13   1.18 2020-03-25 2021-06-18   1.19 2020-08-26 2021-10-28   1.20 2020-12-08 2022-02-28   1.21 2021-04-08 2022-06-28   1.22 2021-08-04 2022-10-28    An initial date refers to the date of first official release of a minor version such as 1.10.0, 1.11.0 … 1.22.0, etc.\nAn EOL date refers to the date of End Of Life (EOL) release, i.e., the community will not maintain this version from then on. This is the final bug-fix release about one year after the initial release.\n","categories":"","description":"","excerpt":"KLTS, known as Kubernetes Long Term Support, has a primary mission to …","ref":"/docs/intro/","tags":"","title":"Overview"},{"body":"KLTS 全称为 Kubernetes Long Term Support，主要使命是为 Kubernetes 早期版本提供长期免费的维护支持。\n之所以需要维护早期版本，是因为在实际生产环境中，最新版本不一定是最好的，也不是最稳定的。正常而言，Kubernetes 社区版本的维护周期只有一年左右，请参阅 Kubernetes 版本发行周期。在社区停止维护后，KLTS 在接下来的三年内提供免费维护服务。\n在实际生产中，为什么大多数企业选择采用早期的 Kubernetes 版本管控集群呢？\n  首先，升级频率高会带来变更风险，每次升级必须进行充分验证。特别是金融行业的平台层变更周期通常比较长，因为一旦升级后的新版本存在 bug，就需要被迫回滚或快速响应升级至更新的版本，这样会造成不必要的成本支出。\n  其次，Kubernetes 升级后部分功能的替代方案还没有完全生产就绪，在生产环境中常会出现不兼容的状况。\n  最后，Kubernetes 社区仅支持小版本 +1 升级，不支持跨版本升级，因为跨版本升级经常会出现一些不可控的因素，造成更大的生产问题。\n  所以大多数企业的选择是沿用早期版本，不会贸然升级。但 Kubernetes 社区只维护最新的 3 到 4 个版本，如何才能保证这些早期版本免受社区不定时发现的 CVE 漏洞和 bug 的袭扰呢？这就是 KLTS 的价值所在！我们对早期版本提供长达 3 年的免费维护支持，积极修复早期版本的 CVE 安全漏洞和重大 bug。\nKLTS 维护周期 Kubernetes 版本号表示为 x.y.z，其中 x 是大版本号，y 是小版本号，z 是补丁版本，KLTS 提供的补丁版本号通常以 lts1、lts2 … ltsn 表示。为了方便表述，本节用前两位 x.y 描述 Kubernetes 版本号。\n假设社区发布的最新 Kubernetes 版本为 x.y，根据社区版本维护声明，社区仅维护最近的三个版本，而 KLTS 目前维护从 1.10 起的近十个早期版本，如下图所示。\n当 Kubernetes 社区发现可能影响生产的 CVE 新漏洞或 bug，受到影响的可能不止是社区正在维护的版本，还有之前已经停止维护、但企业仍在使用、且不能贸然升级的版本，KLTS 团队维护的正是这些社区放弃维护的版本。目前 KLTS 的版本维护周期如下：\n从上图可看出，Kubernetes 社区对某个版本的维护周期通常在一年左右，而 KLTS 可以在接下来的三年内提供长期维护，直至代码无法兼容，才会将相应版本淘汰。\nKLTS 修复范围 有些高优先级的 CVE 或严重 Bug 存在于生产环境中会造成较大的安全隐患。CVE 安全问题是集群的生命线，KLTS 会优先修复中高级别的 CVE，其次会修复重大 Bug，确保生产环境稳定运行。\n以 2021 年 1 月发现的 CVE-2021-3121 安全漏洞为例，CVSS 危急分数高达 7.5。但截止 2021 年 9 月 Kubernetes 社区：\n 仅修复了 4 个版本：1.18、1.19、1.20、1.21 宣称“所有早期版本均有这个安全漏洞，建议用户立即停止使用早期版本” 拒绝修复早期版本漏洞的要求  KLTS 针对这一现状，默默修复了深受 CVE-2021-3121 安全漏洞影响的 8 个早期版本：\n v1.17.17 v1.16.15 v1.15.12 v1.14.10 v1.13.12 v1.12.10 v1.11.10 v1.10.13  如果您觉得 KLTS 团队的付出有价值，让您值得信赖，欢迎任何开发者加入 KLTS 社区交流并做出贡献。\n欢迎携手培育硕果 辛勤的耕耘，最终收获的是美丽花朵和累累硕果。经过开发者精心的维护支持，KLTS 为这些早期版本带来以下成果：\n  三年维护期：Kubernetes 社区对每个版本提供一年左右的维护，而 KLTS 接下来会为该版本提供长达三年的持续维护。\n  安全稳定：小版本升级更安全，兼容性高。渐进式升级稳定性更好。譬如最新版本的新功能也许很有吸引力，但是不一定能达到生产可用的标准，需要较长时间的积累。\n  易于安装：结合国内镜像加速，原生支持 Kubeadm，支持 CentOS、Ubuntu、openSUSE，还提供了一键安装脚本。\n  公开透明：在 GitHub 托管的开源项目，全流程公开。\n  全链路规划：后续会添加 Containerd 及其他组件的长期维护。\n  在此也向广大的开发者，再次发出邀请，如果您觉得 KLTS 团队的付出有价值，让您值得信赖，欢迎任何开发者加入 KLTS 社区交流并贡献，期待您的任何意见、建议或解决方案。\n 加入 KLTS 社区 加入 KLTS Slack 聊天频道  Kubernetes 版本发行周期 Kubernetes 社区最近十多个版本的发布时间统计如下：\n   K8s 版本 初次发布日期 EOL 日期     1.10 2018-03-27 2019-02-13   1.11 2018-07-28 2019-05-01   1.12 2018-09-28 2019-07-08   1.13 2018-12-04 2019-10-15   1.14 2019-03-25 2019-12-11   1.15 2019-07-20 2020-05-06   1.16 2019-09-18 2020-09-02   1.17 2019-12-08 2021-01-13   1.18 2020-03-25 2021-06-18   1.19 2020-08-26 2021-10-28   1.20 2020-12-08 2022-02-28   1.21 2021-04-08 2022-06-28   1.22 2021-08-04 2022-10-28    初次发布：指的是 Kubernetes 初次发布的 0 号版本，即 1.10.0、1.11.0 … 1.22.0 等。\nEOL 全称为 End Of Life，即官方社区结束维护，通常发生在初次发布大约 1 年后，这也是官方社区发布的最后一个 bug fix 版本。\n","categories":"","description":"","excerpt":"KLTS 全称为 Kubernetes Long Term Support，主要使命是为 Kubernetes 早期版本提供长期免费的维护支 …","ref":"/zh/docs/intro/","tags":"","title":"简介"},{"body":"This page describes how you can clone a Kubernetes master branch to your local computer.\nClone single branch Since the repos branch is used as a source package for RPM and DEB, it will be very large to directly clon it, so you shall try to only clone the master branch.\ngit clone --single-branch -b master https://github.com/klts-io/kubernetes-lts ","categories":"","description":"","excerpt":"This page describes how you can clone a Kubernetes master branch to …","ref":"/docs/developer-guide/clone/","tags":"","title":"Clone"},{"body":"This page introduces some preparations before installation. For example, it is required to install the kubeadm toolbox to get started. For information on how to create a cluster with kubeadm, see the Step-By-Step Install page.\nBefore you start You should prepare or perform the following actions:\n A compatible Linux host. The Kubernetes project provides generic instructions for Linux distributions based on Debian and Red Hat, and those distributions without a package manager. 2 GB or more of RAM per host (any less will leave little room for your apps.) 2 CPUs or more per host. Good network connectivity between all nodes in the cluster (either public or on-premise network.) The hostname, MAC address, and product_uuid shall be unique for every node. See here for more details. Open the required ports on the host. See Check required ports for more details. Disable the swap partition. You MUST disable the swap partition to keep the kubelet working properly.  Verify the MAC address and product_uuid Verify the MAC address and product_uuid are unique for every node. Perform the following actions:\n Get the MAC address of the network interfaces using the command ip link or ifconfig -a Check the product_uuid using the command sudo cat /sys/class/dmi/id/product_uuid  It is very likely that hardware devices have unique addresses although some virtual machines may use identical addresses. Kubernetes uses these addresses to uniquely identify the nodes in the cluster. If these addresses are not unique to each node, the installation process may fail due to some issues.\nCheck network adapters If you have more than one network adapter and your Kubernetes components are not reachable via the default route, it is recommended to add IP route(s) so the Kubernetes cluster can set up proper connections via the appropriate adapter.\nEnable iptables discover the bridged traffic Make sure that the br_netfilter module is loaded. This can be done by running lsmod | grep br_netfilter. To load it explicitly, you can run the command sudo modprobe br_netfilter.\nTo enable the iptables on your Linux node to correctly discover the bridged traffic, you should ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config file, for example:\ncat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system For more details see the Network Plugin Requirements page.\nCheck required ports This section lists all ports that you may use on your nodes.\nControl-plane node(s)    Protocol Direction Port Range Purpose Used By     TCP Inbound 6443* Kubernetes API server All   TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd   TCP Inbound 10250 kubelet API Self, Control plane   TCP Inbound 10251 kube-scheduler Self   TCP Inbound 10252 kube-controller-manager Self    Worker node(s)    Protocol Direction Port Range Purpose Used By     TCP Inbound 10250 kubelet API Self, Control plane   TCP Inbound 30000-32767 NodePort Services All    Above is the default port range for NodePort Services.\nAny port numbers marked with * are overridable, so you will need to ensure any custom ports you provide are also open.\nAlthough etcd ports are included in control-plane nodes, you can also host your own etcd cluster externally or on custom ports.\nThe pod network plugin you use (see below) may also require certain ports to be open. Since this differs with each pod network plugin, see the documentation for the plugins about what port(s) they need.\nSet a host name Set a hostname for your host by using the following command:\nhostnamectl set-hostname your-new-host-name echo \"127.0.0.1 $(hostname)\" \u003e\u003e /etc/hosts echo \"::1 $(hostname)\" \u003e\u003e /etc/hosts Disable Swap Run the following command to disable the partition swap:\nswapoff -a If you want to disable swap permanently, edit the /etc/fstab file to comment out the swap mount.\nDisable Selinux Run the following command to disable the selinux:\nsetenforce 0 If you want to disable selinux permanently, edit /etc/sysconfig/selinux and replace SELINUX=enforcing with SELINUX=disabled.\nInstall Runtime To run containers in Pods, Kubernetes uses a Container Runtime Interface (CRI).\nLinux Node Other OS By default, Kubernetes uses a CRI to interface with your chosen container runtime.\nIf you don’t specify a runtime, kubeadm automatically tries to detect an installed container runtime by scanning through a list of well-known Unix domain sockets. The following table lists container runtimes and their associated socket paths:\n   Runtime Path to Unix domain socket     Docker /var/run/dockershim.sock   Containerd /run/containerd/containerd.sock   CRI-O /var/run/crio/crio.sock     If both Docker and Containerd are detected, Docker takes precedence. This is inevitable because Docker 18.09 ships with Containerd and both are detectable even if you only installed Docker. If any other two or more runtimes are detected, kubeadm exits with an error. The kubelet integrates with Docker through the built-in dockershim CRI implementation.\n By default, kubeadm uses Docker as the container runtime. The kubelet integrates with Docker through the built-in dockershim CRI implementation.\n Docker Containerd  Red Hat-based distributions Debian-based distributions Run the following command to install a Red Hat-based distribution:\nyum install docker  Run the following command to install a Debian-based distribution:\napt-get install docker.io   By default, Containerd only provides download packages for the amd64 architecture. If you are using a different architecture, you can install the containerd.io package from the official Docker repository. Instructions for setting up the Docker repository and installing the containerd.io package for your respective Linux distribution can be found in Installing the Docker Engine.\nYou can also use the following source code to build.\nVERSION=1.5.4 wget -c https://github.com/containerd/containerd/releases/download/v${VERSION}/containerd-${VERSION}-linux-amd64.tar.gz tar xvf containerd-${VERSION}-linux-amd64.tar.gz -C /usr/local/ mkdir /etc/containerd/ \u0026\u0026 containerd config default \u003e /etc/containerd/config.toml wget -c -O /etc/systemd/system/containerd.service https://raw.githubusercontent.com/containerd/containerd/main/containerd.service systemctl start containerd \u0026\u0026 systemctl enable containerd  See container runtimes for more information.\n","categories":"","description":"","excerpt":"This page introduces some preparations before installation. For …","ref":"/docs/pre-install/","tags":"","title":"Preparation"},{"body":"本页介绍如何将 Kubernetes master 分支克隆到本地。\n克隆主分支 请尝试只克隆主分支，由于 repos 仓库作为 rpm 和 deb 的软件源，直接克隆全部会非常大。\n可执行以下命令克隆主分支：\ngit clone --single-branch -b main https://github.com/klts-io/kubernetes-lts ","categories":"","description":"","excerpt":"本页介绍如何将 Kubernetes master 分支克隆到本地。\n克隆主分支 请尝试只克隆主分支，由于 repos 仓库作为 rpm …","ref":"/zh/docs/developer-guide/clone/","tags":"","title":"克隆"},{"body":"本页介绍安装 Kubernetes 之前需要做好的一些准备工作，例如先要安装 kubeadm 工具箱。有关在安装此工具箱后如何用其创建集群，请参阅正常安装。\n准备工作  准备一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令。 每台主机至少 2 GB 或更多的内存（如果内存太少将影响应用的运行） CPU 2 核或更多 集群中所有主机的网络连通（公网和内网） 单个节点上不能有重复的主机名、MAC 地址或 product_uuid，请参阅确保每个节点上 MAC 地址和 product_uuid 的唯一性。 开启主机上的某些端口，请参阅检查所需端口。 禁用交换分区。为了保证 kubelet 正常工作，您必须禁用交换分区。  确保每个节点上 MAC 地址和 product_uuid 的唯一性   使用命令 ip link 或 ifconfig -a 来获取网络接口的 MAC 地址 使用 sudo cat /sys/class/dmi/id/product_uuid 命令来校验 product_uuid  一般来讲，硬件设备拥有唯一的地址，但是有些虚拟机的地址可能会重复。 Kubernetes 使用 MAC 地址和 product_uuid 来确定集群中的唯一节点。 如果这些值在每个节点上不唯一，可能会导致安装失败。\n检查网络适配器 如果您有一个以上的网络适配器，同时您的 Kubernetes 组件通过默认路由不可达，我们建议您预先添加 IP 路由规则，这样 Kubernetes 集群就可以通过对应的适配器完成连接。\n允许 iptables 检查桥接流量 确保 br_netfilter 模块被加载。这一操作可以通过运行 lsmod | grep br_netfilter 来完成。若要显式加载该模块，可执行命令 sudo modprobe br_netfilter。\n为了让您的 Linux 节点上的 iptables 能够正确地查看桥接流量，您需要确保在 sysctl 配置中将 net.bridge.bridge-nf-call-iptables 设置为 1。例如：\ncat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 更多细节请查阅网络插件需求页面。\n检查所需端口 控制平面节点    协议 方向 端口范围 作用 使用者     TCP 入站 6443 Kubernetes API 服务器 所有组件   TCP 入站 2379-2380 etcd 服务器客户端 API kube-apiserver、etcd   TCP 入站 10250 Kubelet API kubelet 自身、控制平面组件   TCP 入站 10251 kube-scheduler kube-scheduler 自身   TCP 入站 10252 kube-controller-manager kube-controller-manager 自身    工作节点    协议 方向 端口范围 作用 使用者     TCP 入站 10250 Kubelet API kubelet 自身、控制平面组件   TCP 入站 30000-32767 NodePort 服务 所有组件    以上是 NodePort 服务的默认端口范围。\n使用 * 标记的任意端口号都可以被覆盖，所以您需要保证定制的端口是开放的。\n虽然控制平面节点已经包含了 etcd 的端口，您也可以使用自定义的外部 etcd 集群，或指定自定义端口。\n您使用的 Pod 网络插件 (见下) 也可能需要某些特定端口开启。由于各个 Pod 网络插件都有所不同，请参阅相应文档中的端口要求。\n设置节点名字 命令的语法格式如下：\nhostnamectl set-hostname your-new-host-name echo \"127.0.0.1 $(hostname)\" \u003e\u003e /etc/hosts echo \"::1 $(hostname)\" \u003e\u003e /etc/hosts 关闭 Swap 执行以下命令关闭 Swap：\nswapoff -a 如果需要永久关闭，请编辑 /etc/fstab 文件，将 swap 的挂载路径改为注释。\n关闭 Selinux 执行以下命令关闭 Selinux：\nsetenforce 0 如果需要永久关闭，请编辑 /etc/sysconfig/selinux 将 SELINUX=enforcing 替换为 SELINUX=disabled。\n安装 runtime 为了在 Pod 中运行容器，Kubernetes 使用容器运行时（Container Runtime）。\nLinux 节点 其它操作系统 默认情况下，Kubernetes 使用容器运行时接口（Container Runtime Interface，CRI）来与您所选择的容器运行时交互。\n如果您不指定运行时，则 kubeadm 会自动尝试检测系统上已经安装的运行时，方法是扫描一组众所周知的 Unix 域套接字。\n下面的表格列举了一些容器运行时及其对应的套接字路径：\n   运行时 域套接字     Docker /var/run/dockershim.sock   Containerd /run/containerd/containerd.sock   CRI-O /var/run/crio/crio.sock     如果同时检测到 Docker 和 Containerd，则优先选择 Docker。 这是必然的，即使您仅安装了 Docker，因为 Docker 18.09 附带了 Containerd，所以两者都是可以检测到的。 如果检测到其他两个或多个运行时，则 kubeadm 输出错误信息并退出。 kubelet 通过内置的 dockershim CRI 实现与 Docker 集成。\n 默认情况下， kubeadm 使用 docker 作为容器运行时。kubelet 通过内置的 dockershim CRI 实现与 Docker 集成。\n Docker Containerd  基于 Red Hat 的发行版 基于 Debian 的发行版 执行以下命令安装基于 Red Hat 发行版的 Docker：\nyum install docker  执行以下命令安装基于 Debian 发行版的 Docker：\napt-get install docker.io   Containerd 官方默认只提供 amd64 架构的下载包，如果您采用的是其他基础架构， 可以从 Docker 官方仓库安装 containerd.io 软件包。在安装 Docker 引擎中 找到为各自的 Linux 发行版设置 Docker 存储库和安装 containerd.io 软件包的有关说明。\n也可以使用以下源代码构建。\nVERSION=1.5.4 wget -c https://github.com/containerd/containerd/releases/download/v${VERSION}/containerd-${VERSION}-linux-amd64.tar.gz tar xvf containerd-${VERSION}-linux-amd64.tar.gz -C /usr/local/ mkdir /etc/containerd/ \u0026\u0026 containerd config default \u003e /etc/containerd/config.toml wget -c -O /etc/systemd/system/containerd.service https://raw.githubusercontent.com/containerd/containerd/main/containerd.service systemctl start containerd \u0026\u0026 systemctl enable containerd  参阅容器运行时以了解更多信息。\n","categories":"","description":"","excerpt":"本页介绍安装 Kubernetes 之前需要做好的一些准备工作，例如先要安装 kubeadm 工具箱。有关在安装此工具箱后如何用其创建集群， …","ref":"/zh/docs/pre-install/","tags":"","title":"准备"},{"body":"This page describes how you can install dependencies on different operating systems.\nInstall dependencies MacOS Red Hat-based distributions Debian-based distributions Run the following code to install dependencies:\nbrew install jq git python@3 # Install brew, See https://brew.sh/ pip3 install yq  Run the following code to install dependencies:\nyum install -y epel-release yum install -y jq git python3 pip3 install yq  Run the following code to install dependencies:\napt-get install -y jq git python3 python3-pip pip3 install yq  ","categories":"","description":"","excerpt":"This page describes how you can install dependencies on different …","ref":"/docs/developer-guide/dependent/","tags":"","title":"Dependencies"},{"body":"KLTS provides a script that automates the installation process.\nInstall wget https://github.com/klts-io/klts/raw/main/install.sh chmod +x install.sh ./install.sh Usage: ./install.sh [OPTIONS] -h, --help : Display this help and exit --kubernetes-container-registry=ghcr.io/klts-io/kubernetes-lts : Kubernetes container registry --kubernetes-version=1.18.20-lts.1 : Kubernetes version to install --containerd-version=1.3.10-lts.0 : Containerd version to install --runc-version=1.0.2-lts.0 : Runc version to install --kubernetes-rpm-source=https://github.com/klts-io/kubernetes-lts/raw/rpm-v1.18.20-lts.2 : Kubernetes RPM source --containerd-rpm-source=https://github.com/klts-io/containerd-lts/raw/rpm-v1.3.10-lts.0 : Containerd RPM source --runc-rpm-source=https://github.com/klts-io/runc-lts/raw/rpm-v1.0.2-lts.0 : Runc RPM source --others-rpm-source=https://github.com/klts-io/others/raw/rpm : Other RPM source --kubernetes-deb-source=https://github.com/klts-io/kubernetes-lts/raw/deb-v1.18.20-lts.2 : Kubernetes DEB source --containerd-deb-source=https://github.com/klts-io/containerd-lts/raw/deb-v1.3.10-lts.0 : Containerd DEB source --runc-deb-source=https://github.com/klts-io/runc-lts/raw/deb-v1.0.2-lts.0 : Runc DEB source --others-deb-source=https://github.com/klts-io/others/raw/deb : Other DEB source --focus=enable-iptables-discover-bridged-traffic,disable-swap,disable-selinux,setup-source,install-kubernetes,install-containerd,install-runc,install-crictl,install-cniplugins,setup-crictl-config,setup-containerd-cni-config,setup-kubelet-config,setup-containerd-config,daemon-reload,start-containerd,status-containerd,enable-containerd,start-kubelet,status-kubelet,enable-kubelet,images-pull,control-plane-init,status-nodes,show-join-command : Focus on specific step --skip='' : Skip on specific step ","categories":"","description":"","excerpt":"KLTS provides a script that automates the installation process. …","ref":"/docs/install-with-script/","tags":"","title":"Install with Script"},{"body":"KLTS provides a way to install source packages based on Deb and RPM. You can choose the installation method that suits your system.\nBefore installation, ensure that your Preparation is good enough.\nSet the KLTS source package Red Hat-based distributions Debian-based distributions Run the following code to set the source of downloading a proper distribution:\nVERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/yum.repos.d/klts.repo [klts] name=klts baseurl=https://raw.githubusercontent.com/klts-io/kubernetes-lts/rpm-v${VERSION}/\\$basearch/ enabled=1 gpgcheck=0 [klts-others] name=klts-others baseurl=https://raw.githubusercontent.com/klts-io/others/rpm/\\$basearch/ enabled=1 gpgcheck=0 EOF yum makecache  Run the following code to set the source of downloading a proper distribution:\nVERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/apt/sources.list.d/klts.list deb [trusted=yes] https://raw.githubusercontent.com/klts-io/kubernetes-lts/deb-v${VERSION} stable main deb [trusted=yes] https://raw.githubusercontent.com/klts-io/others/deb stable main EOF apt-get update  Install Red Hat-based distributions Debian-based distributions Run the following code to install a distribution:\nyum install kubeadm kubelet kubectl  Run the following code to install a distribution:\napt-get install kubeadm kubelet kubectl  Auto-start Kubelet on boot Run the following code to start Kubelet on boot:\nsystemctl enable kubelet Pull the dependency image Run the following code to pull the dependency image:\nVERSION=1.18.20-lts.2 REPOS=ghcr.io/klts-io/kubernetes-lts kubeadm config images pull --image-repository ${REPOS} --kubernetes-version v${VERSION} All subsequent operations on Kubeadm need to include --image-repository and --kubernetes-version to actively specify the image.\nInitialize the control plane node Run the following code to initialize the control plane node:\nVERSION=1.18.20-lts.2 REPOS=ghcr.io/klts-io/kubernetes-lts kubeadm init --image-repository ${REPOS} --kubernetes-version v${VERSION} For details see Create a cluster with kubeadm.\n","categories":"","description":"","excerpt":"KLTS provides a way to install source packages based on Deb and RPM. …","ref":"/docs/install/","tags":"","title":"Step-By-Step Install"},{"body":"本页介绍如何在不同的操作系统上安装依赖。\n安装依赖 MacOS 基于 Red Hat 的发行版 基于 Debian 的发行版 运行以下命令安装依赖：\nbrew install jq git python@3 # 安装 brew, 请看 https://brew.sh/ pip3 install yq  运行以下命令安装依赖：\nyum install -y epel-release yum install -y jq git python3 pip3 install yq  运行以下命令安装依赖：\napt-get install -y jq git python3 python3-pip pip3 install yq  ","categories":"","description":"","excerpt":"本页介绍如何在不同的操作系统上安装依赖。\n安装依赖 MacOS 基于 Red Hat 的发行版 基于 Debian 的发行版 运行以下命令安 …","ref":"/zh/docs/developer-guide/dependent/","tags":"","title":"依赖"},{"body":"KLTS 提供了基于 deb 和 rpm 软件源的安装方式，您可以选择适合的安装方式。\n安装前请确认已经完成了准备工作。\n设置 KLTS 软件源 基于 Red Hat 的发行版 基于 Debian 的发行版 基于 Red Hat 的发行版, 国内加速 🚀 基于 Debian 的发行版, 国内加速 🚀 执行以下代码设置下载 KLTS 的软件源：\nVERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/yum.repos.d/klts.repo [klts] name=klts baseurl=https://raw.githubusercontent.com/klts-io/kubernetes-lts/rpm-v${VERSION}/\\$basearch/ enabled=1 gpgcheck=0 [klts-other] name=klts-others baseurl=https://raw.githubusercontent.com/klts-io/others/rpm/\\$basearch/ enabled=1 gpgcheck=0 EOF yum makecache  执行以下代码设置下载 KLTS 的软件源：\nVERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/apt/sources.list.d/klts.list deb [trusted=yes] https://raw.githubusercontent.com/klts-io/kubernetes-lts/deb-v${VERSION} stable main deb [trusted=yes] https://raw.githubusercontent.com/klts-io/others/deb stable main EOF apt-get update   说明：以下加速均来自第三方, 安全和稳定性不做保障, 仅建议测试环境使用 ❗️❗️❗️ 执行以下代码设置下载 KLTS 的软件源： /etc/hosts hub.fastgit.org ghproxy.com raw.githubusercontents.com raw.staticdn.net curl https://raw.githubusercontent.com/wzshiming/github-hosts/master/hosts \u003e\u003e/etc/hosts VERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/yum.repos.d/klts.repo [klts] name=klts baseurl=https://raw.githubusercontent.com/klts-io/kubernetes-lts/rpm-v${VERSION}/\\$basearch/ enabled=1 gpgcheck=0 [klts-other] name=klts-others baseurl=https://raw.githubusercontent.com/klts-io/others/rpm/\\$basearch/ enabled=1 gpgcheck=0 EOF yum makecache  VERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/yum.repos.d/klts.repo [klts] name=klts baseurl=https://hub.fastgit.org/klts-io/kubernetes-lts/raw/rpm-v${VERSION}/\\$basearch/ enabled=1 gpgcheck=0 [klts-other] name=klts-others baseurl=https://hub.fastgit.org/klts-io/others/raw/rpm/\\$basearch/ enabled=1 gpgcheck=0 EOF yum makecache  VERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/yum.repos.d/klts.repo [klts] name=klts baseurl=https://ghproxy.com/https://raw.githubusercontent.com/klts-io/kubernetes-lts/rpm-v${VERSION}/\\$basearch/ enabled=1 gpgcheck=0 [klts-other] name=klts-others baseurl=https://ghproxy.com/https://raw.githubusercontent.com/klts-io/others/rpm/\\$basearch/ enabled=1 gpgcheck=0 EOF yum makecache  VERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/yum.repos.d/klts.repo [klts] name=klts baseurl=https://raw.githubusercontents.com/klts-io/kubernetes-lts/rpm-v${VERSION}/\\$basearch/ enabled=1 gpgcheck=0 [klts-other] name=klts-others baseurl=https://raw.githubusercontents.com/klts-io/others/rpm/\\$basearch/ enabled=1 gpgcheck=0 EOF yum makecache  VERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/yum.repos.d/klts.repo [klts] name=klts baseurl=https://raw.staticdn.net/klts-io/kubernetes-lts/rpm-v${VERSION}/\\$basearch/ enabled=1 gpgcheck=0 [klts-other] name=klts-others baseurl=https://raw.staticdn.net/klts-io/others/rpm/\\$basearch/ enabled=1 gpgcheck=0 EOF yum makecache    说明：以下加速均来自第三方, 安全和稳定性不做保障, 仅建议测试环境使用 ❗️❗️❗️\n执行以下代码设置下载 KLTS 的软件源： /etc/hosts hub.fastgit.org ghproxy.com raw.githubusercontents.com raw.staticdn.net curl https://raw.githubusercontent.com/wzshiming/github-hosts/master/hosts \u003e\u003e/etc/hosts VERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/apt/sources.list.d/klts.list deb [trusted=yes] https://raw.githubusercontent.com/klts-io/kubernetes-lts/deb-v${VERSION} stable main deb [trusted=yes] https://raw.githubusercontent.com/klts-io/others/deb stable main EOF apt-get update  VERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/apt/sources.list.d/klts.list deb [trusted=yes] https://hub.fastgit.org/klts-io/kubernetes-lts/raw/deb-v${VERSION} stable main deb [trusted=yes] https://hub.fastgit.org/klts-io/others/raw/deb stable main EOF apt-get update  VERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/apt/sources.list.d/klts.list deb [trusted=yes] https://ghproxy.com/https://raw.githubusercontent.com/klts-io/kubernetes-lts/deb-v${VERSION} stable main deb [trusted=yes] https://ghproxy.com/https://raw.githubusercontent.com/klts-io/others/deb stable main EOF apt-get update  VERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/apt/sources.list.d/klts.list deb [trusted=yes] https://raw.githubusercontents.com/klts-io/kubernetes-lts/deb-v${VERSION} stable main deb [trusted=yes] https://raw.githubusercontents.com/klts-io/others/deb stable main EOF apt-get update  VERSION=1.18.20-lts.2 cat \u003c\u003c EOF \u003e /etc/apt/sources.list.d/klts.list deb [trusted=yes] https://raw.staticdn.net/klts-io/kubernetes-lts/deb-v${VERSION} stable main deb [trusted=yes] https://raw.staticdn.net/klts-io/kubernetes-lts/deb stable main EOF apt-get update   安装 基于 Red Hat 的发行版 基于 Debian 的发行版 执行以下命令安装：\nyum install kubeadm kubelet kubectl  执行以下命令安装：\napt-get install kubeadm kubelet kubectl  开机自动启动 Kubelet 执行以下命令开机自动启动 Kubelet：\nsystemctl enable kubelet 拉取依赖镜像 默认 国内加速 🚀 执行以下命令 pull 依赖的镜像：\nVERSION=1.18.20-lts.2 REPOS=ghcr.io/klts-io/kubernetes-lts kubeadm config images pull --image-repository ${REPOS} --kubernetes-version v${VERSION}  执行以下命令 pull 依赖的镜像：\nVERSION=1.18.20-lts.2 REPOS=ghcr.m.daocloud.io/klts-io/kubernetes-lts kubeadm config images pull --image-repository ${REPOS} --kubernetes-version v${VERSION}  后续对 kubeadm 的操作都需要加上 --image-repository 和 --kubernetes-version 以主动指定镜像。\n初始化控制面节点 默认 国内加速 🚀 执行以下命令初始化控制面的节点：\nVERSION=1.18.20-lts.2 REPOS=ghcr.io/klts-io/kubernetes-lts kubeadm init --image-repository ${REPOS} --kubernetes-version v${VERSION}  执行以下命令初始化控制面的节点：\nVERSION=1.18.20-lts.2 REPOS=ghcr.m.daocloud.io/klts-io/kubernetes-lts kubeadm init --image-repository ${REPOS} --kubernetes-version v${VERSION}  有关更多安装说明，请参阅 Kubernetes 操作指南。\n","categories":"","description":"","excerpt":"KLTS 提供了基于 deb 和 rpm 软件源的安装方式，您可以选择适合的安装方式。\n安装前请确认已经完成了准备工作。\n设置 KLTS 软 …","ref":"/zh/docs/install/","tags":"","title":"正常安装"},{"body":"KLTS 提供了以下脚本自动完成安装流程。\nwget https://github.com/klts-io/klts/raw/main/install.sh chmod +x install.sh ./install.sh Usage: ./install.sh [OPTIONS] -h, --help : Display this help and exit --kubernetes-container-registry=ghcr.io/klts-io/kubernetes-lts : Kubernetes container registry --kubernetes-version=1.18.20-lts.1 : Kubernetes version to install --containerd-version=1.3.10-lts.0 : Containerd version to install --runc-version=1.0.2-lts.0 : Runc version to install --kubernetes-rpm-source=https://github.com/klts-io/kubernetes-lts/raw/rpm-v1.18.20-lts.2 : Kubernetes RPM source --containerd-rpm-source=https://github.com/klts-io/containerd-lts/raw/rpm-v1.3.10-lts.0 : Containerd RPM source --runc-rpm-source=https://github.com/klts-io/runc-lts/raw/rpm-v1.0.2-lts.0 : Runc RPM source --others-rpm-source=https://github.com/klts-io/others/raw/rpm : Other RPM source --kubernetes-deb-source=https://github.com/klts-io/kubernetes-lts/raw/deb-v1.18.20-lts.2 : Kubernetes DEB source --containerd-deb-source=https://github.com/klts-io/containerd-lts/raw/deb-v1.3.10-lts.0 : Containerd DEB source --runc-deb-source=https://github.com/klts-io/runc-lts/raw/deb-v1.0.2-lts.0 : Runc DEB source --others-deb-source=https://github.com/klts-io/others/raw/deb : Other DEB source --focus=enable-iptables-discover-bridged-traffic,disable-swap,disable-selinux,setup-source,install-kubernetes,install-containerd,install-runc,install-crictl,install-cniplugins,setup-crictl-config,setup-containerd-cni-config,setup-kubelet-config,setup-containerd-config,daemon-reload,start-containerd,status-containerd,enable-containerd,start-kubelet,status-kubelet,enable-kubelet,images-pull,control-plane-init,status-nodes,show-join-command : Focus on specific step --skip='' : Skip on specific step ","categories":"","description":"","excerpt":"KLTS 提供了以下脚本自动完成安装流程。\nwget …","ref":"/zh/docs/install-with-script/","tags":"","title":"脚本一键安装"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/containerd/patches/","tags":"","title":"Fixed CVEs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/patches/","tags":"","title":"Fixed CVEs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/containerd/patches/","tags":"","title":"修复的漏洞"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/patches/","tags":"","title":"修复的漏洞"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/","tags":"","title":"Kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/","tags":"","title":"Kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/containerd/releases/","tags":"","title":"Release log"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/kubernetes/releases/","tags":"","title":"Release log"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/containerd/releases/","tags":"","title":"版本日志"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/kubernetes/releases/","tags":"","title":"版本日志"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/containerd/","tags":"","title":"Containerd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/containerd/","tags":"","title":"Containerd"},{"body":"CVE 在 kubernetes 社区是持续更新的，我们需要在维护的所有版本中修复更新的 CVE 漏洞。基于源 patch 文件在低于此 patch 的版本更新出各个版本对应的 patch 文件。klts 有自动更新 patch 的功能，我们只需要解决冲突，保证代码的有效性和可用性。\n下面将介绍如何把在 klts 中添加一个 CVE 补丁。\n 先指定需要修复的 CVE 脚本，修改 release.yaml，先添加对应 CVE 的 patch 来源，此路径可以是一个链接，也可以下载到 /patches 目录，Score 是代表这个 CVE 的优先级，数值越高说明对安全的影响更大。\u003c k8s1.13 表示在1.13（1.12，1.11，1.10）以下的版本都需要被解决漏洞。  在1.12版本中运用 CVE-2019-11253 patch 文件：\n- name: v1.12.10-lts.1 base_release: v1.12.10-ci must: true patches: - CVE-2019-11253 # CVSS Score 5.0, \u003c k8s1.13, https://www.cvedetails.com/cve/CVE-2019-11253/ # TODO - name: CVE-2019-11253 patch: - https://github.com/kubernetes/kubernetes/pull/83436.patch 检查 v1.12.10-lts.1 是否能够完全运用此 patch，如果有冲突，将会手动去解决冲突代码。  ~/workspace/kubernetes-lts: main ± : make v1.12.10-lts.1 Checkout to v1.12.10-lts.1 ./hack/checkout.sh v1.12.10-lts.1 remote: Enumerating objects: 77218, done. remote: Counting objects: 100% (29330/29330), done. remote: Compressing objects: 100% (11622/11622), done. remote: Total 14180 (delta 7451), reused 5859 (delta 2116), pack-reused 0 Receiving objects: 100% (14180/14180), 15.77 MiB | 1.90 MiB/s, done. Resolving deltas: 100% (7451/7451), completed with 4241 local objects. Reset branch 'tag-v1.12.10' 如果输出下面的情况则表示这些文件有冲突：\nTo restore the original branch and stop patching, run \"git am --abort\". +++ Conflicts detected: UU Godeps/Godeps.json UU staging/src/k8s.io/api/Godeps/Godeps.json UU staging/src/k8s.io/apiextensions-apiserver/Godeps/Godeps.json UU staging/src/k8s.io/apimachinery/Godeps/Godeps.json UU staging/src/k8s.io/apiserver/Godeps/Godeps.json UU staging/src/k8s.io/cli-runtime/Godeps/Godeps.json UU staging/src/k8s.io/client-go/Godeps/Godeps.json UU staging/src/k8s.io/csi-api/Godeps/Godeps.json UU staging/src/k8s.io/kube-aggregator/Godeps/Godeps.json UU staging/src/k8s.io/metrics/Godeps/Godeps.json UU staging/src/k8s.io/sample-apiserver/Godeps/Godeps.json UU staging/src/k8s.io/sample-cli-plugin/Godeps/Godeps.json UU staging/src/k8s.io/sample-controller/Godeps/Godeps.json UU vendor/gopkg.in/yaml.v2/decode.go UU vendor/gopkg.in/yaml.v2/encode.go Aborting. +++ Aborting in-progress git am. +++ Returning you to the tag-v1.12.10 branch and cleaning up. make: *** [v1.12.10-lts.1] Error 1  如果出现没法读取到某一个 patch 文件，请确保源 patch 文件正常下载到 /tmp 目录，请重新执行 make 命令。\n 根据输出的信息，重新修改 1.12 的 patch 信息：  +++ About to attempt patch. To reattempt: $ git am -3 /path/to/kubernetes-lts/tmp/83436.patch Applying: bump gopkg.in/yaml.v2 v2.2.4 Using index info to reconstruct a base tree... M\tGodeps/Godeps.json M\tstaging/src/k8s.io/api/Godeps/Godeps.json M\tstaging/src/k8s.io/apiextensions-apiserver/Godeps/Godeps.json M\tstaging/src/k8s.io/apimachinery/Godeps/Godeps.json M\tstaging/src/k8s.io/apiserver/Godeps/Godeps.json M\tstaging/src/k8s.io/cli-runtime/Godeps/Godeps.json M\tstaging/src/k8s.io/client-go/Godeps/Godeps.json A\tstaging/src/k8s.io/cloud-provider/Godeps/Godeps.json M\tstaging/src/k8s.io/csi-api/Godeps/Godeps.json M\tstaging/src/k8s.io/kube-aggregator/Godeps/Godeps.json M\tstaging/src/k8s.io/metrics/Godeps/Godeps.json M\tstaging/src/k8s.io/sample-apiserver/Godeps/Godeps.json M\tstaging/src/k8s.io/sample-cli-plugin/Godeps/Godeps.json M\tstaging/src/k8s.io/sample-controller/Godeps/Godeps.json M\tvendor/gopkg.in/yaml.v2/decode.go M\tvendor/gopkg.in/yaml.v2/encode.go M\tvendor/gopkg.in/yaml.v2/resolve.go M\tvendor/gopkg.in/yaml.v2/scannerc.go 将 patch 复制到/patches目录：\ncp /path/to/kubernetes-lts/tmp/83436.patch patches/CVE-2019-11253.1.12.patch 重新修改 release.yaml 文件：\n- name: v1.12.10-lts.1 base_release: v1.12.10-ci must: true patches: - CVE-2019-11253.1.12 # CVSS Score 5.0, \u003c k8s1.13, https://www.cvedetails.com/cve/CVE-2019-11253/ # TODO - name: CVE-2019-11253 patch: - https://github.com/kubernetes/kubernetes/pull/83436.patch - name: CVE-2019-11253.1.12 patch: - patches/CVE-2019-11253.1.12.patch 手动解决冲突文件，请注意手动解决并不是只是简单的代码替换，要满足下面几个条件：    采用 patch 修改的内容\n  上下文没有语法错误\n  必须能通过测试用例\n  执行下面命令解决冲突文件：\nQUIET=n ./hack/format_patch.sh patches/CVE-2019-11253.1.12.patch ... +++ Conflicts detected: UU Godeps/Godeps.json UU staging/src/k8s.io/api/Godeps/Godeps.json UU staging/src/k8s.io/apiextensions-apiserver/Godeps/Godeps.json UU staging/src/k8s.io/apimachinery/Godeps/Godeps.json UU staging/src/k8s.io/apiserver/Godeps/Godeps.json UU staging/src/k8s.io/cli-runtime/Godeps/Godeps.json UU staging/src/k8s.io/client-go/Godeps/Godeps.json UU staging/src/k8s.io/csi-api/Godeps/Godeps.json UU staging/src/k8s.io/kube-aggregator/Godeps/Godeps.json UU staging/src/k8s.io/metrics/Godeps/Godeps.json UU staging/src/k8s.io/sample-apiserver/Godeps/Godeps.json UU staging/src/k8s.io/sample-cli-plugin/Godeps/Godeps.json UU staging/src/k8s.io/sample-controller/Godeps/Godeps.json UU vendor/gopkg.in/yaml.v2/decode.go UU vendor/gopkg.in/yaml.v2/encode.go +++ Please resolve the conflicts in another window (and remember to 'git add / git am --continue') +++ Proceed (anything but 'y' aborts the patch)? [y/n] 终端会暂停在冲突阶段，请注意此时需要重新打开另一个终端去解决冲突文件，待手动解决完所有冲突进行检查：\n~/path/to/kubernetes-lts/src/github.com/kubernetes/kubernetes:: tag-v1.12.10 ±✚ \u003eR\u003e : git add . ~/path/to/kubernetes-lts/src/github.com/kubernetes/kubernetes:: tag-v1.12.10 ✚ \u003eR\u003e : git am --continue Applying: Limit YAML/JSON decode size ~/path/to/kubernetes-lts/src/github.com/kubernetes/kubernetes:: tag-v1.12.10 : git status On branch tag-v1.12.10 nothing to commit, working tree clean 最重要的一步操作就是返回之前的终端，确认冲突已经完成，此时会去更新 patches/CVE-2019-11253.1.12.patch。\n以上操作就是更新一个版本的 patch 整个操作，可以选择先提交到仓库跑 CI 的测试，如果当前版本的测试用例都通过了就说明此次修复有效。然后选择在此 patch 上向下兼容。修改 release.yaml 文件:  - name: v1.12.10-lts.1 base_release: v1.12.10-ci must: true patches: - CVE-2019-11253.1.12 - name: v1.11.10-lts.1 base_release: v1.11.10-ci must: true patches: - CVE-2019-11253.1.11 # CVSS Score 5.0, \u003c k8s1.13, https://www.cvedetails.com/cve/CVE-2019-11253/ # TODO - name: CVE-2019-11253 patch: - https://github.com/kubernetes/kubernetes/pull/83436.patch - name: CVE-2019-11253.1.12 patch: - patches/CVE-2019-11253.1.12.patch - name: CVE-2019-11253.1.11 patch: - patches/CVE-2019-11253.1.11.patch 继续执行 make v1.11.10-lts.1，如果有冲突的话，重复以上动作修复。直到验证所最后一个版本（v1.10.13-lts.1）。\n当所有的冲突解决完以后，并不意味代码将可以正常使用，必须通过 CI 的测试用例。目前 CI 只支持 main 分支。如果在自己的仓库，请在 main 分支进行修改，确保完全无误的情况下再提交 PR 申请合入 klts 主分支。  在 github 查看测试情况：\n查看提交的所有 job 信息，下面说明 1.15.12 版本测试将不通过：\n查看报错信息详情，将错误解决完后重新提交，直到通过所有的测试。\n","categories":"","description":"","excerpt":"CVE 在 kubernetes 社区是持续更新的，我们需要在维护的所有版本中修复更新的 CVE 漏洞。基于源 patch …","ref":"/zh/docs/addpatch/add-patch/","tags":"","title":"添加 CVE 补丁到 klts"},{"body":"我们很高兴宣布 KLTS 开源项目正式上线了。KLTS 适用于企业用户，可以直接用于生产环境。在 Kubernetes 社区停止维护某个版本后，KLTS 将继续提供两到三年的维护，主要包括修补 CVE 漏洞和较为严重的 bug。\nKLTS 从提出思路、代码整理、构建加速通道、建设网站，到最后发布上线，共用了 2 个多月时间。我们的目的是分享和交流，分享使人快乐，交流让人进步。\n为什么会有 KLTS 这个开源项目呢？是因为 Kubernetes 官方社区仅维护最新的 3 - 4 个版本，而许多实际生产环境采用了以前的旧版本，如果贸然升级到新版本，容易引发生产故障。\n我们就遇到过这样一个案例。\n升级难题 2021 年 7 月份，某个银行客户将 Kubernetes 1.11 直接升级到 1.18，升级后进行业务系统压力测试时，生产环境出现严重故障。\n该银行客户管理层和我们公司领导都非常重视，立即组织专家们加班加点复现问题，对两个跨省的大生产集群逐节点排查，反复进行三次尝试，终于找对了问题的方向，成功复现故障发生的临界点。\n多番测试后，最终确定是新老集群节点迁移期间因更换 Master 节点导致新集群网络出现大规模故障，旧 Master 支持的某些功能在升级后没有得到很好地支持。\n事后银行高管反馈说：“之所以需要升级旧系统，是因为 Kubernetes 1.11 的某个安全漏洞，但是万万没想到升级后会出现这种大范围的故障。”\n内部反思 发生这样的问题，往往会影响实际的生产业务，引发一系列严重后果。所以这次事件之后，我们内部进行了详细的思考讨论。意识到生产环境中运行的许多旧版本 Kubernetes 并不能贸然升级到新版本，这很容易引发各种各样无法预料的问题，客户也可能因此陷于进退两难的境地。\n为了提高客户满意度，我们这两年来持续维护 Kubernetes 1.10 起的各个版本，解决各种各样的故障问题。经常有同事深夜接到 on-call 售后服务电话，用冷水洗一把脸，就立即开始远程排查，再抬头时天都快亮了。\n两年多的日夜奋战，我们修复了许多 bug，解决了数不清的故障。这次事件之后，大家意识到这些维护过的 Kubernetes 版本是很多企业求而不得的数字资产。\n团队内部提出一个大胆的建议：“这些版本是否能分享到社区，让更多用户受益？而不仅仅是局限于咱公司范围的客户，这样也能为社区的发展略尽绵薄之力。”\n正逢 Kubernetes Community Day (KCD) 大会如火如荼地展开，这一提议得到了团队成员的积极响应。\n项目构建 说干就干，首先是从公司代码仓库上扒出历年修复的补丁，依托于 Github 的 CI，恢复测试，构建软件源 (YUM/DEB) 和镜像，建设网站，编写文档。\n现已经在 CentOS7 经过了较为充分的测试。\n正式发布 2021 年 11 月，金秋送爽，一切准备就绪，KLTS 正式对外发布。我们不求这个开源项目多么大红大紫，只求同事们几年来积累的经验能够为您带来一些帮助，让您少走弯路。\n从 1.10 起所有内核都是久经金融政企生产验证的软件，您可以放心下载，经过测试验证后就能直接投入生产环境。\n如您有任何意见或疑问，请立即联系我们。\n 加入 KLTS’s Slack 加入 KLTS 社区  ","categories":"","description":"","excerpt":"我们很高兴宣布 KLTS 开源项目正式上线了。KLTS 适用于企业用户，可以直接用于生产环境。在 Kubernetes 社区停止维护某个版本 …","ref":"/zh/blog/2021/11/17/klts-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/","tags":"","title":"KLTS 正式发布"},{"body":"We are happy to announce that the KLTS project is officially launched. KLTS is suitable for enterprise users and can be directly used in a production environment. After the Kubernetes community stops maintaining a certain version, KLTS will continue to provide maintenance for two to three years, mainly including patching CVE vulnerabilities and fixing serious bugs.\nKLTS team took more than 2 months from proposing ideas, organizing code, building acceleration channels and websites, and finally launching it online. Our purpose is to share and communicate, sharing makes people happy, and communication makes progress.\nWhy do we need the KLTS project? The reason is that the official Kubernetes community only maintains the latest 3 - 4 versions, but those earlier versions are still used in many actual production environments. If you rashly upgrade to a new version, it is easy to cause production failures.\nWe have encountered such a case.\nUpgrade issues In July 2021, a bank customer directly upgraded Kubernetes 1.11 to 1.18. When the customer performed a stress test in the upgraded business system, a serious failure occurred in the production environment.\nThe bank’s management and our company’s leaders both showed great considerations to this matter, and immediately organized experts to work overtime and try to reproduce the failure situation again. The two large inter-provincial production clusters were investigated node by node. After repeated attempts, they finally found the right direction and succeeded. They successfully located the critical point at which the failure occurs.\nAfter several tests, it was finally determined that the migration of the Master node from old to new cluster nodes caused a large-scale failure of the new cluster network. Some features supported by the old Master were not well supported after the upgrade.\nAfterwards, a bank executive reported: “The reason for upgrading the old system is because of a security vulnerability in Kubernetes 1.11, but we never expected such a large-scale failure after the upgrade.”\nInternal discussion The occurrence of such failures often affects the actual production and triggers a series of serious consequences. So after this incident, we conducted detailed internal discussions and realized that earlier versions of Kubernetes running in the production environment cannot be rushed to upgrade to the new version, which can easily cause various unforeseen problems and customers may also be in a dilemma.\nIn order to improve customer satisfaction, we have continued to maintain those earlier versions of Kubernetes from 1.10 over the past two years to solve different failures. Often a colleague received an after-sales service call late at night, immediately started remote investigation after quickly cleaning his face with cold water, and outside was almost bright when he solved the problem and looked out of the window again.\nAfter fighting day and night for more than two years, we fixed many bugs and solved countless failures. After this bank incident, our team realized that these maintained versions of Kubernetes are digital assets that many enterprises cannot ask for.\n”The team put forward a big suggestion: “Can these versions be shared with the open source community and benefit more users? Not just limited to our company’s customers, it can also contribute some to the community development.”\nJust as the Kubernetes Community Day (KCD) conference was held globally, this proposal received positive responses from our team members.\nBuild Do whatever it takes. The first step is to pick up the patches that have been fixed over the years from our code repository. Then rely on Github’s CI, resume testing, build software sources (YUM/DEB), images, and websites, and write documents.\nAll these releases have been fully tested in CentOS7.\nOfficial release In November 2021, it is the golden autumn season. Once everything is ready, KLTS is officially released. We hope that the experience accumulated by our colleagues over the past few years can bring you some help and you can avoid detours.\nSince 1.10, all kernels have been verified by financial, government, and enterprise production environment. You can download it with confidence and directly put it into the production environment after being simple tested and verified.\nIf you have any comments or questions, feel free to reach out to us in the following ways:\n Slack KLTS community  ","categories":"","description":"","excerpt":"We are happy to announce that the KLTS project is officially launched. …","ref":"/blog/2021/11/17/official-release-of-klts/","tags":"","title":"Official release of KLTS"},{"body":"Welcome to the new KLTS blog space.\nHere you can keep up with the progress of the KLTS open source project and recent hot topics.\nWe also plan to include release notes for major releases, guidance articles, community-related events, and possibly some development tips, and interesting topics within the team.\nIf you are using a version of k8s provided by KLTS or are interested in contributing to this open source project and would like to join the discussion or make some guest blog posts, please contact us.\n","categories":"","description":"","excerpt":"Welcome to the new KLTS blog space.\nHere you can keep up with the …","ref":"/blog/2021/10/15/welcome-to-the-klts-blog/","tags":"","title":"Welcome to the KLTS Blog!"},{"body":"欢迎来到 KLTS 的专属博客空间。\n您可以在这里了解 KLTS 开源项目的进展和最近的热议话题。\n我们还计划在这里列出主要版本的发布说明、指导文章、社区有关事件，可能还会有一些开发的心得体会和团队内部的有趣话题。\n如果您正在使用 KLTS 提供的 k8s 版本或者有意为这个开源项目做贡献，想要加入讨论或发表一些客座博文，请联系我们。\n","categories":"","description":"","excerpt":"欢迎来到 KLTS 的专属博客空间。\n您可以在这里了解 KLTS 开源项目的进展和最近的热议话题。\n我们还计划在这里列出主要版本的发布说明、 …","ref":"/zh/blog/2021/10/15/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0-klts-%E7%9A%84%E5%8D%9A%E5%AE%A2/","tags":"","title":"欢迎来到 KLTS 的博客"},{"body":"  #td-cover-block-0 { background-image: url(/about/featured-background_hucee3cb3223e1b5244391c4cde6cfd3d7_469584_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/about/featured-background_hucee3cb3223e1b5244391c4cde6cfd3d7_469584_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  About KLTS KLTS (Kubernetes Long Term Support)        Kubernetes being an enterprise infrastructure, you should not use a version that is no longer maintained. KLTS maintains the versions that Kubernetes community drops officially. Simply upgrading to the KLTS patch version can enable you avoid bugs caused by newer official versions of Kubernetes, no worry about introducing features that are not currently available, and your infrastructure based on Kubernetes will be more stable.\n    The KLTS process is fully hosted on GitHub, and you can simply Fork the project and build your own version of Kubernetes. The build artifacts are all stored on GitHub, the images in the GitHub Package, and the RPM and Deb packages in the repos branch of the same repository.      KLTS will maintain a release for at least two years after the official end of maintenance, to mainly patch CVE vulnerabilities and more serious bugs.     ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/about/","tags":"","title":"About KLTS"},{"body":"This is the exclusive blog space for KLTS.\n","categories":"","description":"","excerpt":"This is the exclusive blog space for KLTS.\n","ref":"/blog/","tags":"","title":"Blog"},{"body":"","categories":"","description":"","excerpt":"","ref":"/case-studies/","tags":"","title":"Case Studies"},{"body":"","categories":"","description":"","excerpt":"","ref":"/case-studies/daocloud/","tags":"","title":"DaoCloud Case Study"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/case-studies/daocloud/","tags":"","title":"DaoCloud 案例"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/developer-guide/","tags":"","title":"Developer Guide"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"Document"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hucee3cb3223e1b5244391c4cde6cfd3d7_469584_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hucee3cb3223e1b5244391c4cde6cfd3d7_469584_1920x1080_fill_q75_catmullrom_top.jpg); } }  Welcome to KLTS.IO Lean More   Source Code   Long term support of Kubernetes is available here          KLTS offers production distributions for earlier versions of Kubernetes, which are fully open source distributions that include complete Kubernetes environment and dependencies\n      DaoCloud Community  We provide open source toolkits for image verification, interactive UI design, document management, and more.\nRead more …\n   Welcome to join KLTS!  We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Learn more about DaoCloud  A innovative leader of enterprise-level PaaS provider\nRead more …\n    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/","tags":"","title":"KLTS.io"},{"body":"  #td-cover-block-0 { background-image: url(/zh/featured-background_hucee3cb3223e1b5244391c4cde6cfd3d7_469584_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/featured-background_hucee3cb3223e1b5244391c4cde6cfd3d7_469584_1920x1080_fill_q75_catmullrom_top.jpg); } }  欢迎来到 KLTS.IO 阅读文档   查看源码   稳定、长期维护的 Kubernetes 早期版本都在这里\n         KLTS 持续维护 Kubernetes 早期发行的版本，定期修复常见的 CVE 漏洞和 bug，可直接用于生产，完全开源，包含了完整的 Kubernetes 运行时环境及其依赖\n      DaoCloud 开源社区  提供镜像验证、交互设计、运维套件和文档开发等实用的开源工具！\n更多 …\n   欢迎加入 KLTS  我们在 GitHub 上开放了 Pull Request 贡献工作流。欢迎开发者加入！\n更多 …\n   了解 DaoCloud  企业级云计算领域的创新领导者\n更多 …\n    ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/","tags":"","title":"KLTS.io"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/search/","tags":"","title":"Search Results"},{"body":"  #td-cover-block-0 { background-image: url(/zh/about/featured-background_hucee3cb3223e1b5244391c4cde6cfd3d7_469584_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/zh/about/featured-background_hucee3cb3223e1b5244391c4cde6cfd3d7_469584_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  关于 KLTS KLTS (Kubernetes Long Term Support)        Kubernetes 是一个企业级容器集群管理系统，但目前社区仅维护最新的几个版本。 如果您使用的是较早的版本，该怎么办呢？不用担心，KLTS 帮助您维护社区不再维护的版本。 我们目前持续维护 1.10 到 1.18 近 10 个版本，您只需下载对应的版本，就能获得稳定运行的 Kubernetes 并享受持续维护的免费服务。\n    KLTS 所有流程完全托管在 GitHub 上，您可以直接 Fork 项目构建属于自己的 Kubernetes 版本。 所有镜像存放在 GitHub Package，而 rpm 和 deb 包存放在同仓库的 repos 分支。      Kubernetes 社区停止维护某个版本后，KLTS 将继续提供两年以上的维护，主要包括修补 CVE 漏洞和较为严重的 bug。      ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/zh/about/","tags":"","title":"关于 KLTS"},{"body":"这是 KLTS 的专属博客空间。\n","categories":"","description":"","excerpt":"这是 KLTS 的专属博客空间。\n","ref":"/zh/blog/","tags":"","title":"博客"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/developer-guide/","tags":"","title":"开发指南"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/docs/","tags":"","title":"文档"},{"body":"","categories":"","description":"","excerpt":"","ref":"/zh/case-studies/","tags":"","title":"用户案例"}]